{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "WhzRGjVz1IaPwqZdjKbNKo",
     "report_properties": {
      "rowId": "NRE4THkPQvp5DIRltrNpV9"
     },
     "type": "MD"
    }
   },
   "source": [
    "# Déployez un modèle dans le cloud\n",
    "\n",
    "\n",
    "# Sommaire :\n",
    "\n",
    "**1. Préambule**<br />\n",
    "&emsp;1.1 Problématique<br />\n",
    "&emsp;1.2 Objectifs dans ce projet<br />\n",
    "&emsp;1.3 Déroulement des étapes du projet<br />\n",
    "**2. Choix techniques généraux retenus**<br />\n",
    "&emsp;2.1 Calcul distribué<br />\n",
    "&emsp;2.2 Transfert Learning<br />\n",
    "**3. Déploiement de la solution en local**<br />\n",
    "&emsp;3.1 Environnement de travail<br />\n",
    "&emsp;3.2 Installation de Spark<br />\n",
    "&emsp;3.3 Installation des packages<br />\n",
    "&emsp;3.4 Import des librairies<br />\n",
    "&emsp;3.5 Définition des PATH pour charger les images et enregistrer les résultats<br />\n",
    "&emsp;3.6 Création de la SparkSession<br />\n",
    "&emsp;3.7 Traitement des données<br />\n",
    "&emsp;&emsp;3.7.1 Chargement des données<br />\n",
    "&emsp;&emsp;3.7.2 Préparation du modèle<br />\n",
    "&emsp;&emsp;3.7.3 Définition du processus de chargement des images et application <br />\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;de leur featurisation à travers l'utilisation de pandas UDF<br />\n",
    "&emsp;&emsp;3.7.4 Exécution des actions d'extractions de features<br />\n",
    "&emsp;3.8 Chargement des données enregistrées et validation du résultat<br />\n",
    "**4. Déploiement de la solution sur le cloud**<br />\n",
    "&emsp;4.1 Choix du prestataire cloud : AWS<br />\n",
    "&emsp;4.2 Choix de la solution technique : EMR<br />\n",
    "&emsp;4.3 Choix de la solution de stockage des données : Amazon S3<br />\n",
    "&emsp;4.4 Configuration de l'environnement de travail<br />\n",
    "&emsp;4.5 Upload de nos données sur S3<br />\n",
    "&emsp;4.6 Configuration du serveur EMR<br />\n",
    "&emsp;&emsp;4.6.1 Étape 1 : Logiciels et étapes<br />\n",
    "&emsp;&emsp;&emsp;4.6.1.1 Configuration des logiciels<br />\n",
    "&emsp;&emsp;&emsp;4.6.1.2 Modifier les paramètres du logiciel<br />\n",
    "&emsp;&emsp;4.6.2 Étape 2 : Matériel<br />\n",
    "&emsp;&emsp;4.6.3 Étape 3 : Paramètres de cluster généraux<br />\n",
    "&emsp;&emsp;&emsp;4.6.3.1 Options générales<br />\n",
    "&emsp;&emsp;&emsp;4.6.3.2 Actions d'amorçage<br />\n",
    "&emsp;&emsp;4.6.4 Étape 4 : Sécurité<br />\n",
    "&emsp;&emsp;&emsp;4.6.4.1 Options de sécurité<br />\n",
    "&emsp;4.7 Instanciation du serveur<br />\n",
    "&emsp;4.8 Création du tunnel SSH à l'instance EC2 (Maître)<br />\n",
    "&emsp;&emsp;4.8.1 Création des autorisations sur les connexions entrantes<br />\n",
    "&emsp;&emsp;4.8.2 Création du tunnel ssh vers le Driver<br />\n",
    "&emsp;&emsp;4.8.3 Configuration de FoxyProxy<br />\n",
    "&emsp;&emsp;4.8.4 Accès aux applications du serveur EMR via le tunnel ssh<br />\n",
    "&emsp;4.9 Connexion au notebook JupyterHub<br />\n",
    "&emsp;4.10 Exécution du code<br />\n",
    "&emsp;&emsp;4.10.1 Démarrage de la session Spark<br />\n",
    "&emsp;&emsp;4.10.2 Installation des packages<br />\n",
    "&emsp;&emsp;4.10.3 Import des librairies<br />\n",
    "&emsp;&emsp;4.10.4 Définition des PATH pour charger les images et enregistrer les résultats<br />\n",
    "&emsp;&emsp;4.10.5 Traitement des données<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.1 Chargement des données<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.2 Préparation du modèle<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.3 Définition du processus de chargement des images<br />\n",
    "&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;et application de leur featurisation à travers l'utilisation de pandas UDF<br />\n",
    "&emsp;&emsp;&emsp;4.10.5.4 Exécutions des actions d'extractions de features<br />\n",
    "&emsp;&emsp;4.10.6 Chargement des données enregistrées et validation du résultat<br />\n",
    "&emsp;4.11 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark<br />\n",
    "&emsp;4.12 Résiliation de l'instance EMR<br />\n",
    "&emsp;4.13 Cloner le serveur EMR (si besoin)<br />\n",
    "&emsp;4.14 Arborescence du serveur S3 à la fin du projet<br />\n",
    "**5. Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "7IukX7q65BbYegoagdkSp2",
     "report_properties": {
      "rowId": "ESjeOrKVMiDXPW51npSGNL"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 1. Préambule\n",
    "\n",
    "## 1.1 Problématique\n",
    "\n",
    "La très jeune start-up de l'AgriTech, nommée \"**Fruits**!\", <br />\n",
    "cherche à proposer des solutions innovantes pour la récolte des fruits.\n",
    "\n",
    "La volonté de l’entreprise est de préserver la biodiversité des fruits <br />\n",
    "en permettant des traitements spécifiques pour chaque espèce de fruits <br />\n",
    "en développant des robots cueilleurs intelligents.\n",
    "\n",
    "La start-up souhaite dans un premier temps se faire connaître en mettant <br />\n",
    "à disposition du grand public une application mobile qui permettrait aux <br />\n",
    "utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
    "\n",
    "Pour la start-up, cette application permettrait de sensibiliser le grand public <br /> \n",
    "à la biodiversité des fruits et de mettre en place une première version du moteur <br />\n",
    "de classification des images de fruits.\n",
    "\n",
    "De plus, le développement de l’application mobile permettra de construire <br />\n",
    "une première version de l'architecture **Big Data** nécessaire.\n",
    "\n",
    "## 1.2 Objectifs dans ce projet\n",
    "\n",
    "1. Développer une première chaîne de traitement des données qui <br />\n",
    "   comprendra le **preprocessing** et une étape de **réduction de dimension**.\n",
    "2. Tenir compte du fait que <u>le volume de données va augmenter <br />\n",
    "   très rapidement</u> après la livraison de ce projet, ce qui implique de:\n",
    " - Déployer le traitement des données dans un environnement **Big Data**\n",
    " - Développer les scripts en **pyspark** pour effectuer du **calcul distribué**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5k4Q5dQAWoYBEsQ5azzJ5R",
     "report_properties": {
      "rowId": "b24rP3Ki3daHa8eUNL3zRI"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 1.3 Déroulement des étapes du projet\n",
    "\n",
    "Le projet va être réalisé en 2 temps, dans deux environnements différents. <br />\n",
    "Nous allons dans un premier temps développer et exécuter notre code en local, <br />\n",
    "en travaillant sur un nombre limité d'images à traiter.\n",
    "\n",
    "Une fois les choix techniques validés, nous déploierons notre solution <br />\n",
    "dans un environnement Big Data en mode distribué.\n",
    "\n",
    "<u>Pour cette raison, ce projet sera divisé en 3 parties</u>:\n",
    "1. Liste des choix techniques généraux retenus\n",
    "2. Déploiement de la solution en local\n",
    "3. Déploiement de la solution dans le cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "TLKCrfiZZ8DfJP83VM1zAx",
     "report_properties": {
      "rowId": "LEoj54KhvKnuCTswaL8eAU"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 2. Choix techniques généraux retenus"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "bGZolScnuElPlItJ1EiO72",
     "report_properties": {
      "rowId": "6LxNYUtbUBGnMbx33FPlq6"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 2.1 Calcul distribué\n",
    "\n",
    "L’énoncé du projet nous impose de développer des scripts en **pyspark** <br />\n",
    "afin de <u>prendre en compte l’augmentation très rapide du volume <br />\n",
    "de donné après la livraison du projet</u>.\n",
    "\n",
    "Pour comprendre rapidement et simplement ce qu’est **pyspark** <br />\n",
    "et son principe de fonctionnement, nous vous conseillons de lire <br />\n",
    "cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
    "\n",
    "<u>Le début de l’article nous dit ceci </u>:<br />\n",
    "« *Lorsque l’on parle de traitement de bases de données sur python, <br />\n",
    "on pense immédiatement à la librairie pandas. Cependant, lorsqu’on a <br />\n",
    "affaire à des bases de données trop massives, les calculs deviennent trop lents.<br />\n",
    "Heureusement, il existe une autre librairie python, assez proche <br />\n",
    "de pandas, qui permet de traiter des très grandes quantités de données : PySpark.<br />\n",
    "Apache Spark est un framework open-source développé par l’AMPLab <br />\n",
    "de UC Berkeley permettant de traiter des bases de données massives <br />\n",
    "en utilisant le calcul distribué, technique qui consiste à exploiter <br />\n",
    "plusieurs unités de calcul réparties en clusters au profit d’un seul <br />\n",
    "projet afin de diviser le temps d’exécution d’une requête.<br />\n",
    "Spark a été développé en Scala et est au meilleur de ses capacités <br />\n",
    "dans son langage natif. Cependant, la librairie PySpark propose de <br />\n",
    "l’utiliser avec le langage Python, en gardant des performances <br />\n",
    "similaires à des implémentations en Scala.<br />\n",
    "Pyspark est donc une bonne alternative à la librairie pandas lorsqu’on <br />\n",
    "cherche à traiter des jeux de données trop volumineux qui entraînent <br />\n",
    "des calculs trop chronophages.* »\n",
    "\n",
    "Comme nous le constatons, **pySpark** est un moyen de communiquer <br />\n",
    "avec **Spark** via le langage **Python**.<br />\n",
    "**Spark**, quant à lui, est un outil qui permet de gérer et de coordonner <br />\n",
    "l'exécution de tâches sur des données à travers un groupe d'ordinateurs. <br />\n",
    "<u>Spark (ou Apache Spark) est un framework open source de calcul distribué <br />\n",
    "in-memory pour le traitement et l'analyse de données massives</u>.\n",
    "\n",
    "Un autre [article très intéressant et beaucoup plus complet pour <br />\n",
    "comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le rôle <br />\n",
    "des **Spark Session** que nous utiliserons dans ce projet.\n",
    "\n",
    "<u>Voici également un extrait</u>:\n",
    "\n",
    "*Les applications Spark se composent d’un pilote (« driver process ») <br />\n",
    "et de plusieurs exécuteurs (« executor processes »). Il peut être configuré <br />\n",
    "pour être lui-même l’exécuteur (local mode) ou en utiliser autant que <br />\n",
    "nécessaire pour traiter l’application, Spark prenant en charge la mise <br />\n",
    "à l’échelle automatique par une configuration d’un nombre minimum <br />\n",
    "et maximum d’exécuteurs.*\n",
    "\n",
    "![Schéma de Spark](Yil00/Deployez_un_model_dans_le_Cloud_AWS/img/spark-schema.png)\n",
    "\n",
    "*Le driver (parfois appelé « Spark Session ») distribue et planifie <br />\n",
    "les tâches entre les différents exécuteurs qui les exécutent et permettent <br />\n",
    "un traitement réparti. Il est le responsable de l’exécution du code <br />\n",
    "sur les différentes machines.\n",
    "\n",
    "Chaque exécuteur est un processus Java Virtual Machine (JVM) distinct <br />\n",
    "dont il est possible de configurer le nombre de CPU et la quantité de <br />\n",
    "mémoire qui lui est alloué. <br />\n",
    "Une seule tâche peut traiter un fractionnement de données à la fois.*\n",
    "\n",
    "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** <br />\n",
    "et nous l’exploiterons à travers des scripts python grâce à **PySpark**.\n",
    "\n",
    "Dans la <u>version locale</u> de notre script nous **simulerons <br />\n",
    "le calcul distribué** afin de valider que notre solution fonctionne.<br />\n",
    "Dans la <u>version cloud</u> nous **réaliserons les opérations sur un cluster de machine**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "BSGbXO75TUWwyLPt463cj9",
     "report_properties": {
      "rowId": "iEIKIRMzrAJrZWUlgQ7EVp"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 2.2 Transfert Learning\n",
    "\n",
    "L'énoncé du projet nous demande également de <br />\n",
    "réaliser une première chaîne de traitement <br />\n",
    "des données qui comprendra le preprocessing et <br />\n",
    "une étape de réduction de dimension.\n",
    "\n",
    "Il est également précisé qu'il n'est pas nécessaire <br />\n",
    "d'entraîner un modèle pour le moment.\n",
    "\n",
    "Nous décidons de partir sur une solution de **transfert learning**.\n",
    "\n",
    "Simplement, le **transfert learning** consiste <br />\n",
    "à utiliser la connaissance déjà acquise <br />\n",
    "par un modèle entraîné (ici **MobileNetV2**) pour <br />\n",
    "l'adapter à notre problématique.\n",
    "\n",
    "Nous allons fournir au modèle nos images, et nous allons <br />\n",
    "<u>récupérer l'avant dernière couche</u> du modèle.<br />\n",
    "En effet la dernière couche de modèle est une couche softmax <br />\n",
    "qui permet la classification des images ce que nous ne <br />\n",
    "souhaitons pas dans ce projet.\n",
    "\n",
    "L'avant dernière couche correspond à un **vecteur <br />\n",
    "réduit** de dimension (1,1,1280).\n",
    "\n",
    "Cela permettra de réaliser une première version du moteur <br />\n",
    "pour la classification des images des fruits.\n",
    "\n",
    "**MobileNetV2** a été retenu pour sa <u>rapidité d'exécution</u>, <br />\n",
    "particulièrement adaptée pour le traitement d'un gros volume <br />\n",
    "de données ainsi que la <u>faible dimensionnalité du vecteur <br />\n",
    "de caractéristique en sortie</u> (1,1,1280)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "gjDRvXCW76xBPMx7GmeYkl",
     "report_properties": {
      "rowId": "riqgh1CKWDXhV64jskDWSO"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 3. Déploiement de la solution en local\n",
    "\n",
    "\n",
    "## 3.1 Environnement de travail\n",
    "\n",
    "Pour des raisons de simplicité, nous développons dans un environnement <br />\n",
    "Linux Unbuntu (exécuté depuis une machine Windows dans une machine virtuelle)\n",
    "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
    "\n",
    "## 3.2 Installation de Spark\n",
    "\n",
    "[La première étape consiste à installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
    "\n",
    "## 3.3 Installation des packages\n",
    "\n",
    "<u>On installe ensuite à l'aide de la commande **pip** <br />\n",
    "les packages qui nous seront nécessaires</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PB2KIe4TCTusXo7xgTZNSf",
     "report_properties": {
      "rowId": "li8QTVoU5WhpKYa3hUb2st"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install Pandas pillow tensorflow pyspark pyarrow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "28cnEBSNa2GAkmEObKrNIb",
     "report_properties": {
      "rowId": "o84yjCGwMnND50DCo0bfLy"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 3.4 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "CaVtA7xHwPVmeh3B9MTYzJ",
     "report_properties": {
      "rowId": "bNb0jl7O7gADqKxpwMjf0y"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-12 11:00:41.564251: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-12 11:00:46.036241: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-04-12 11:00:46.050514: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-12 11:00:52.945619: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "DLgHfJXWmTsarmYPGwDCKM",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting findspark\n",
      "  Downloading findspark-2.0.1-py2.py3-none-any.whl (4.4 kB)\n",
      "Installing collected packages: findspark\n",
      "Successfully installed findspark-2.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install findspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "AkeNfxwgPkydyUhUwzYLmT",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spark\n",
      "  Downloading spark-0.2.1.tar.gz (41 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: spark\n",
      "  Building wheel for spark (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for spark: filename=spark-0.2.1-py3-none-any.whl size=58748 sha256=53517372ea822e3d9281aa4987ad3f9983144c17ee0c09394955022f3eeaf42a\n",
      "  Stored in directory: /home/yildiz2/.cache/pip/wheels/89/3a/8d/19221e83efd8f57618e9a4171bbd9184347bb060723f33a38d\n",
      "Successfully built spark\n",
      "Installing collected packages: spark\n",
      "Successfully installed spark-0.2.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#pip install spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "fs6QCBh47I8pmiKxfBEOaR",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#Spark -- test 01:lib\n",
    "# Spark.\n",
    "import findspark\n",
    "import spark\n",
    "findspark.init()\n",
    "\n",
    "# Pyspark.\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Spark -- test 02:lib\n",
    "# ML.\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# DL.\n",
    "#from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "# Functions.\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "GAOTlOKBTUfXbKIzT3AzXl",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting now\n",
      "  Downloading now-0.0.6.tar.gz (4.2 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: now\n",
      "  Building wheel for now (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for now: filename=now-0.0.6-py2.py3-none-any.whl size=5058 sha256=91c85358658b278d03cf9d91ca9f1f34a9cfdb12403a5bd04d7fe1351d1d82d8\n",
      "  Stored in directory: /home/yildiz2/.cache/pip/wheels/fe/de/8e/d46c088b85ef22fafc818dbee10ab4c47385bc80752bd56a77\n",
      "Successfully built now\n",
      "Installing collected packages: now\n",
      "Successfully installed now-0.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "hrNvsOvYAS1vPb4zgzvRmZ",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Version des librairies utilisées :\n",
      "Python        : 3.10.9 (main, Mar  1 2023, 18:23:06) [GCC 11.2.0]\n",
      "tensorflow    : 2.12.0\n",
      "pyspark       : 3.3.2\n",
      "PIL           : 9.4.0\n",
      "Numpy         : 1.23.5\n",
      "Pandas        : 1.5.3\n",
      "Matplotlib    : 3.7.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import now\n",
    "import datetime\n",
    "import matplotlib\n",
    "import sys\n",
    "# Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import element_at, split, col, pandas_udf, PandasUDFType, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Tensorflow Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Gestion des images\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Taches ML\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "# Réduction de dimension - PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "# Modélisation\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Versions\n",
    "print('Version des librairies utilisées :')\n",
    "print('Python        : ' + sys.version)\n",
    "print('tensorflow    : ' + tf.__version__)\n",
    "print('pyspark       : ' + pyspark.__version__)\n",
    "print('PIL           : ' + PIL.__version__)\n",
    "print('Numpy         : ' + np.__version__)\n",
    "print('Pandas        : ' + pd.__version__)\n",
    "print('Matplotlib    : ' + matplotlib.__version__)\n",
    "#now = datetime.now().isoformat()\n",
    "#print('Lancé le      : ' + now)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "cGT1yV8lVpJSs1ACYrowv4",
     "report_properties": {
      "rowId": "PN4jmLjolbEzw06xql6QL4"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 3.5 Définition des PATH pour charger les images <br /> et enregistrer les résultats\n",
    "\n",
    "Dans cette version locale nous partons du principe que les données <br />\n",
    "sont stockées dans le même répertoire que le notebook.<br />\n",
    "Nous n'utilisons qu'un extrait de **300 images** à traiter dans cette <br />\n",
    "première version en local.<br />\n",
    "L'extrait des images à charger est stockée dans le dossier **Test1**.<br />\n",
    "Nous enregistrerons le résultat de notre traitement <br />\n",
    "dans le dossier \"**Results_Local**\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "e1VYRMhnulHnyMGg6zKl4l",
     "report_properties": {
      "rowId": "8RvaOppzFoRNK8WH05Ha4A"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        /home/yildiz2/Téléchargements\n",
      "PATH_Data:   /home/yildiz2/Téléchargements/test-multiple_fruits\n",
      "PATH_Result: /home/yildiz2/Téléchargements/Results/home/yildiz2/Téléchargements/test-multiple_fruits\n",
      "PATH_Result: /home/yildiz2/Téléchargements/Results2\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "PATH_Data = PATH+'/test-multiple_fruits' \n",
    "PATH_Result = PATH+'/Results'\n",
    "PATH_Result2 = PATH+'/Results2'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result +\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "IkqA3PlEUcV6yYCPqRraDC",
     "report_properties": {
      "rowId": "8n6gxiawCrzz06idgofC50"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 3.6 Création de la SparkSession\n",
    "\n",
    "L’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé **SparkSession**. <br />\n",
    "<u>Une instance de **SparkSession** est la façon dont Spark exécute les fonctions définies par l’utilisateur <br />\n",
    "dans l’ensemble du cluster</u>. <u>Une SparkSession correspond toujours à une application Spark</u>.\n",
    "\n",
    "<u>Ici nous créons une session spark en spécifiant dans l'ordre</u> :\n",
    " 1. un **nom pour l'application**, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
    " 2. que l'application doit s'exécuter **localement**. <br />\n",
    "   Nous ne définissons pas le nombre de cœurs à utiliser (comme .master('local[4]) pour 4 cœurs à utiliser), <br />\n",
    "   nous utiliserons donc tous les cœurs disponibles dans notre processeur.<br />\n",
    " 3. une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** <br />\n",
    "   que nous utiliserons pour enregistrer et charger le résultat de notre travail.\n",
    " 4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en créer une nouvelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "TSMkQ4PGXT9V7fwNiyjXwR",
     "report_properties": {
      "rowId": "HVYmono8zYSlHEErQ7rUta"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "spark = (SparkSession\n",
    "             .builder\n",
    "             .appName('P8')\n",
    "             .master('local')\n",
    "             .config(\"spark.sql.parquet.writeLegacyFormat\", 'true')\n",
    "             .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "CCDRvxRrV6IF4n78HkvLm3",
     "report_properties": {
      "rowId": "R7QvJ6u5N2kKdx6UgFgGAe"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Nous créons également la variable \"**sc**\" qui est un **SparkContext** issue de la variable **spark**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Dw97GBBN2tk3unYNPYTzuM",
     "report_properties": {
      "rowId": "OoNTBltfCVtWqYFcM9n4Ux"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dWtLElDzunBRR1vTuJrzX7",
     "report_properties": {
      "rowId": "BqZ4kOvP90UUulI67ePBee"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Affichage des informations de Spark en cours d'execution</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "YHAhYWX9BUhJRJLW0iVyaf",
     "report_properties": {
      "rowId": "sBnwZWYy9IaG1nkUv493YK"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-0-207-18:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>P8</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "TCnpYmfjrZpNhE0HaAAEYg",
     "report_properties": {
      "rowId": "cEbeJp2gTf920L6LJzZdLF"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 3.7 Traitement des données\n",
    "\n",
    "<u>Dans la suite de notre flux de travail, <br />\n",
    "nous allons successivement</u> :\n",
    "1. Préparer nos données\n",
    "    1. Importer les images dans un dataframe **pandas UDF**\n",
    "    2. Associer aux images leur **label**\n",
    "    3. Préprocesser en **redimensionnant nos images pour <br />\n",
    "       qu'elles soient compatibles avec notre modèle**\n",
    "2. Préparer notre modèle\n",
    "    1. Importer le modèle **MobileNetV2**\n",
    "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
    "3. Définir le processus de chargement des images et l'application <br />\n",
    "   de leur featurisation à travers l'utilisation de pandas UDF\n",
    "3. Exécuter les actions d'extraction de features\n",
    "4. Enregistrer le résultat de nos actions\n",
    "5. Tester le bon fonctionnement en chargeant les données enregistrées\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Bt45VvZAFn1Iw8QyXdB7A6",
     "report_properties": {
      "rowId": "P9az7rBvZ1Wbx8dusAGgkw"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 3.7.1 Chargement des données\n",
    "\n",
    "Les images sont chargées au format binaire, ce qui offre, <br />\n",
    "plus de souplesse dans la façon de prétraiter les images.\n",
    "\n",
    "Avant de charger les images, nous spécifions que nous voulons charger <br />\n",
    "uniquement les fichiers dont l'extension est **jpg**.\n",
    "\n",
    "Nous indiquons également de charger tous les objets possibles contenus <br />\n",
    "dans les sous-dossiers du dossier communiqué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "rdUzImH7cA1evnteiH0nnM",
     "report_properties": {
      "rowId": "KTExCjkL68rpINxVpoefwN"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5d36zlx9P5NNcfomuLocUv",
     "report_properties": {
      "rowId": "5IDl1ej26nxt6rwrLAX38O"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Affichage des 5 premières images contenant</u> :\n",
    " - le path de l'image\n",
    " - la date et heure de sa dernière modification\n",
    " - sa longueur\n",
    " - son contenu encodé en valeur hexadécimal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Er6Ec2VNaulLlGFOuSWJ4R",
     "report_properties": {
      "rowId": "inqw2oBQrWdpkBSTDiXcnZ"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "aIQEmovBvLLwP1xCvrIwKK",
     "report_properties": {
      "rowId": "MLNJuB5sIOMqsARn60hNZc"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------------------------------------------------------------+-----------------+\n",
      "|path                                                            |label            |\n",
      "+----------------------------------------------------------------+-----------------+\n",
      "|file:/data/notebook_files/test-multiple_fruits/cherries.jpg     |cherries.jpg     |\n",
      "|file:/data/notebook_files/test-multiple_fruits/strawberries.jpg |strawberries.jpg |\n",
      "|file:/data/notebook_files/test-multiple_fruits/apples4.jpg      |apples4.jpg      |\n",
      "|file:/data/notebook_files/test-multiple_fruits/strawberries5.jpg|strawberries5.jpg|\n",
      "|file:/data/notebook_files/test-multiple_fruits/plums_pears.jpg  |plums_pears.jpg  |\n",
      "+----------------------------------------------------------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-1))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "qAuASRkcz0DhPC5YYsErlF",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "ggg3h22BsOhk4gHJsojfmi",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "#path2 = \"/data/notebook_files/test-multiple_fruits\"\n",
    "#df_training = pd.read_csv(path2)\n",
    "# Print the schema.\n",
    "#df_training.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "0dFTZIU60o40spDfao9TX1",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# -- Test-001\n",
    "\n",
    "#path_train_set = \"/data/notebook_files/test-multiple_fruits\"\n",
    "# Chargement au format 'image' des images du train set\n",
    "# avec option inferSchema pour déduire automatiquement les types de colonnes\n",
    "# en fonction des données\n",
    "#df_img_train = spark.read.format('image').load(path_train_set, inferschema=True) \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DUhnSFl0hfbxfx04yHxCP5",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# -- Test -002\n",
    "#images('image.height','image.width').take(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "diu39Xl692uL06UERHucDX",
     "report_properties": {
      "rowId": "vaXa65XFHi2pLSQQnfBkH2"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 3.7.2 Préparation du modèle\n",
    "\n",
    "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\n",
    "J'ai choisi d'utiliser le modèle **MobileNetV2** pour sa rapidité d'exécution comparée <br />\n",
    "à d'autres modèles comme *VGG16* par exemple.\n",
    "\n",
    "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\n",
    "je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
    "\n",
    "<u>Voici le schéma de son architecture globale</u> : \n",
    "\n",
    "![Architecture de MobileNetV2](Deployez_un_model_dans_le_Cloud_AWS/img/mobilenetv2_architecture.png)\n",
    "\n",
    "Il existe une dernière couche qui sert à classer les images <br />\n",
    "selon 1000 catégories que nous ne voulons pas utiliser.<br />\n",
    "L'idée dans ce projet est de récupérer le **vecteur de caractéristiques <br />\n",
    "de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur <br />\n",
    "de classification à reconnaitre les différents fruits du jeu de données.\n",
    "\n",
    "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\n",
    "en incluant toutes ses couches, attend obligatoirement des images <br />\n",
    "de dimension (224,224,3). Nos images étant toutes de dimension (100,100,3), <br />\n",
    "nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
    "\n",
    "<u>Dans l'odre</u> :\n",
    " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** <br />\n",
    "    issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
    " 2. Nous créons un nouveau modèle avec:\n",
    "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2\n",
    "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ZhYLN1Ipx60i8rr6vSB6Jb",
     "report_properties": {
      "rowId": "9xRg4EZqu8HK6r62mltiak"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "omOMAdJpPXSQjro82322n8",
     "report_properties": {
      "rowId": "NbhxnAPgrHPkkWxFvpMcAT"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "bjWmxmPdAiDH9NAtI8kuu6",
     "report_properties": {
      "rowId": "JI30ppVzT4m1AQmYkmWf0d"
     },
     "type": "MD"
    }
   },
   "source": [
    "Affichage du résumé de notre nouveau modèle où nous constatons <br />\n",
    "que <u>nous récupérons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "koPOcWdEHMmbmMTC8eDh5k",
     "report_properties": {
      "rowId": "FrsvBjSWjyVWmNvCaFOCYE"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d_1 (Gl  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " obalAveragePooling2D)                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "z9l7zZEA4w48MzppSbYgxe",
     "report_properties": {
      "rowId": "lqPhDUIrkTW3gD2gY6yHRq"
     },
     "type": "MD"
    }
   },
   "source": [
    "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. <br />\n",
    "Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser <br />\n",
    "ensuite les poids aux différents workeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "1Rfnotw7ugokvRN9V5gNpd",
     "report_properties": {
      "rowId": "7Z4rJCFrbURpiff2PeZkRk"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "td29uIiWAwMrWs8N8PaPdH",
     "report_properties": {
      "rowId": "tl2MphSQU31TY4KZAB2vgm"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Mettons cela sous forme de fonction</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Eiy9R4EbxUeXalaasdvOEp",
     "report_properties": {
      "rowId": "2cqoVUI8NGXWBqWQB71Avu"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Zd80AiDte8uBEQ8NXIOpSp",
     "report_properties": {
      "rowId": "CA6v7VNVpBjCAunkJcN2hp"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 3.7.3 Définition du processus de chargement des images et application <br/>de leur featurisation à travers l'utilisation de pandas UDF\n",
    "\n",
    "Ce notebook définit la logique par étapes, jusqu'à Pandas UDF.\n",
    "\n",
    "<u>L'empilement des appels est la suivante</u> :\n",
    "\n",
    "- Pandas UDF\n",
    "  - featuriser une série d'images pd.Series\n",
    "   - prétraiter une image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "oWdX8MtfZLqGIi510FYLyE",
     "report_properties": {
      "rowId": "df8K26Fb4AFQu977R8oJ5r"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/python/envs/default/lib/python3.8/site-packages/pyspark/sql/pandas/functions.py:394: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "iA1wa4sAZ92MXCgFwzGRTF",
     "report_properties": {
      "rowId": "Ti681pC8A0kDf8n45ROyyB"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 3.7.4 Exécution des actions d'extraction de features\n",
    "\n",
    "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), <br />\n",
    "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
    "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
    "essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
    "\n",
    "Je n'utiliserai pas cette commande dans ce projet <br />\n",
    "et je laisse donc la commande en commentaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "6xiiDrhSrARRTfBSDlH5JR",
     "report_properties": {
      "rowId": "w4GxYexKmQoAm2Mf2rx8zu"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "9VNIvLdqZ1eUv23zwrhqdE",
     "report_properties": {
      "rowId": "3UuWIbJ7YAsYnahESI7jje"
     },
     "type": "MD"
    }
   },
   "source": [
    "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
    "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. <br />\n",
    "\n",
    "Notre jeu de données de **Test** contient **22819 images**. <br />\n",
    "Cependant, dans l'exécution en mode **local**, <br />\n",
    "nous <u>traiterons un ensemble réduit de **330 images**</u>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PzQr4Fu6XJJKeQgezZJHNr",
     "report_properties": {
      "rowId": "VG7ei0eOlw5c2lube4SQzf"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "features_df = images.repartition(20).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "BiwHuqsE3ZOqTJocKkffqA",
     "report_properties": {
      "rowId": "cYNi5p3zHBiZuBo1E7MUOZ"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Rappel du PATH où seront inscrits les fichiers au format \"**parquet**\" <br />\n",
    "contenant nos résultats, à savoir, un DataFrame contenant 3 colonnes</u> :\n",
    " 1. Path des images\n",
    " 2. Label de l'image\n",
    " 3. Vecteur de caractéristiques de l'image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "p4HIWe47nKZEX2axHNbkCm",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "features_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "T9RXn0UBMfpYCSIY5oaEOr",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|                path|               label|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|file:/data/notebo...|          pears2.jpg|[0.0012995078, 0....|\n",
      "|file:/data/notebo...|           dates.jpg|[0.0010436894, 0....|\n",
      "|file:/data/notebo...|      raspberry2.jpg|[0.16309968, 1.03...|\n",
      "|file:/data/notebo...|       chestnut1.jpg|[0.0014374888, 0....|\n",
      "|file:/data/notebo...|   cherries_wax7.jpg|[0.25825173, 0.40...|\n",
      "|file:/data/notebo...|         apples4.jpg|[2.051978, 2.6198...|\n",
      "|file:/data/notebo...|    apples_pears.jpg|[0.0, 1.4148519, ...|\n",
      "|file:/data/notebo...|    raspberries3.jpg|[0.16309968, 1.03...|\n",
      "|file:/data/notebo...|apple_apricot_nec...|[0.56099874, 0.01...|\n",
      "|file:/data/notebo...|   cherries_wax1.jpg|[0.60749894, 0.18...|\n",
      "|file:/data/notebo...|     plums_pears.jpg|[0.8002806, 0.669...|\n",
      "|file:/data/notebo...| apples_peaches2.jpg|[0.0, 1.5745357E-...|\n",
      "|file:/data/notebo...|     pomegranate.jpg|[1.1276621, 0.0, ...|\n",
      "|file:/data/notebo...|       cherries4.jpg|[0.08947858, 0.80...|\n",
      "|file:/data/notebo...|cherries(rainier)...|[1.3608732, 1.830...|\n",
      "|file:/data/notebo...|   cherries_wax5.jpg|[0.0, 0.0, 0.0, 0...|\n",
      "|file:/data/notebo...|    pomegranate2.jpg|[3.678786, 0.0, 0...|\n",
      "|file:/data/notebo...|       cherries8.jpg|[0.015279731, 0.8...|\n",
      "|file:/data/notebo...|apple_apricot_pea...|[0.99363333, 0.0,...|\n",
      "|file:/data/notebo...|grape_pear_mandar...|[0.21920842, 0.57...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "3N7aYXhHYLhmMXD2ZWkvoo",
     "type": "MD"
    }
   },
   "source": [
    "\n",
    "3.7.5. Réduction de dimension - Principal Component Analysis\n",
    "\n",
    "3.7.6. Recherche meilleur nombre de composante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "h4xBiQh6QS6dWSxyRdonD7",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_pca(dataframe):\n",
    "  '''\n",
    "     Préparation des données :\n",
    "     - transformation en vecteur dense\n",
    "     - standardisation\n",
    "     param : dataframe : dataframe d'images\n",
    "     return : dataframe avec features vecteur dense standardisé\n",
    "  '''\n",
    "  \n",
    "  # Préparation des données - conversion des données images en vecteur dense\n",
    "  transform_vecteur_dense = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "  dataframe = dataframe.withColumn('features_vectors', transform_vecteur_dense('features'))\n",
    "  \n",
    "  # Standardisation obligatoire pour PCA\n",
    "  scaler_std = StandardScaler(inputCol=\"features_vectors\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "  model_std = scaler_std.fit(dataframe)\n",
    "  # Mise à l'échelle\n",
    "  dataframe = model_std.transform(dataframe)\n",
    "  \n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "RUg2v9akyuhZJGFlALiZP6",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Recherche du nombre de composante expliquant 95% de la variance\n",
    "def recherche_nb_composante(dataframe, nb_comp=400):\n",
    "    '''\n",
    "       Recherche d nombre de composante expliquant 95% de la variance\n",
    "       param : dataframe : dataframe d'images\n",
    "       return : k nombre de composante expliquant 95% de la variance totale\n",
    "    '''\n",
    "    \n",
    "    pca = PCA(k = nb_comp,\n",
    "              inputCol=\"features_scaled\", \n",
    "              outputCol=\"features_pca\")\n",
    " \n",
    "    model_pca = pca.fit(dataframe)\n",
    "    variance = model_pca.explainedVariance\n",
    " \n",
    "    # visuel\n",
    "    plt.plot(np.arange(len(variance)) + 1, variance.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"Nb composantes\")\n",
    "    plt.ylabel(\"% variance\")\n",
    "    plt.show(block=False)\n",
    " \n",
    "    def nb_comp ():\n",
    "      for i in range(500):\n",
    "          a = variance.cumsum()[i]\n",
    "          if a >= 0.95:\n",
    "              print(\"{} composantes principales expliquent au moins 95% de la variance totale\".format(i))\n",
    "              break\n",
    "      return i\n",
    " \n",
    "    k=nb_comp()\n",
    "  \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "tmpnJAti84Z4uPs6eSlhTu",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Pré-processing (vecteur dense, standardisation)\n",
    "df_pca = preprocess_pca(features_df)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1nK1n7jlVTkWt8GqXnNM5Y",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 composantes principales expliquent au moins 95% de la variance totale\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA8fUlEQVR4nO3dd3TUVf7/8dcESAKEJIRAEiASkCY/KQqCkUXwS9a4a4GNCEK+S9EFuyhFwYbgaqwcULFhQf1aaEF0RSxIFN2IAgbEEomACSVBQBJ6YHJ/f8xm1oGUmWT6PB/nzCHz+dyZeV8Gzevcz733YzHGGAEAAASJMF8XAAAA4E6EGwAAEFQINwAAIKgQbgAAQFAh3AAAgKBCuAEAAEGFcAMAAIJKQ18X4G0VFRXatWuXmjVrJovF4utyAACAE4wxOnjwoFq3bq2wsJrHZkIu3OzatUvJycm+LgMAANRBUVGR2rZtW2ObkAs3zZo1k2T7y4mOjvZxNQAAwBllZWVKTk62/x6vSciFm8pLUdHR0YQbAAACjDNTSphQDAAAggrhBgAABBXCDQAACCqEGwAAEFQINwAAIKgQbgAAQFAh3AAAgKBCuAEAAEGFcAMAAIJKyO1QjBBjtUo5OdKnn0rbt0vGOJ4PC5OSk6W4OGn/fqmw0PU27ngPPsdzn+NPtfA5/v05/lRLIH/OgQO254MG2R4NGsjrjA999tln5rLLLjNJSUlGklm2bFmtr1m9erU555xzTHh4uDnzzDPNK6+84tJnlpaWGkmmtLS0bkXD/508acwnnxgzbJgxkZHG2P4z5MGDBw8e3n60aGHM0qVu+V+7K7+/fXpZ6vDhw+rZs6fmzZvnVPtt27bp0ksv1UUXXaS8vDzddttt+sc//qEPP/zQw5UiIFit0v33S1FRUlqatGSJdOyYr6sCgNC1b5905ZVSdrZXP9ZijDFe/cRqWCwWLVu2TEOHDq22zZ133qn3339fmzdvth+7+uqrdeDAAa1cudKpzykrK1NMTIxKS0u5cWYwWbJEGj1aOnrU15UAAE7Vtq1takA9LlG58vs7oCYU5+bmKi0tzeFYenq6cnNzq33N8ePHVVZW5vBAELFapREjpKuuItgAgL/asUNas8ZrHxdQ4aa4uFgJCQkOxxISElRWVqaj1fxiy8rKUkxMjP2RnJzsjVLhaVarNGuW1LSptGiRr6sBANRm926vfVRAhZu6mD59ukpLS+2PoqIiX5eE+lqyxDYjf8YM6fhxX1cDAHBGUpLXPiqgloInJiaqpKTE4VhJSYmio6PVuHHjKl8TERGhiIgIb5QHb5g6VXr8cV9XAQBwRdu20oABXvu4gAo3qampWrFihcOxjz/+WKmpqT6qCF5jtUpXX20btQEABJa5c726341PL0sdOnRIeXl5ysvLk2Rb6p2Xl6fCwkJJtktKo0ePtre//vrrtXXrVt1xxx366aef9Mwzz2jRokW6/fbbfVE+vGXJEtvyboINAASWFi2kpUuljAyvfqxPR27WrVuniy66yP580qRJkqQxY8ZowYIF2r17tz3oSFL79u31/vvv6/bbb9fcuXPVtm1bvfjii0pPT/d67fASd12GathQSk21DY3+kb/u8MnnBGctfI5/f44/1RLIn+MHOxT7zT433sI+NwFk8mRp9uz6vUejRtJdd0n33uubLcABAG7hyu/vgJpzgxBhtUojR0qLF9fvfYYPl958k1ADACEm6JeCI8AsWSI1b16/YBMdbdv7ZuFCgg0AhCDCDfzH1Km2nYYPHqzb688/X/rkE9s14auucm9tAICAwWUp+If6zq+ZNEl64gn31QMACFiM3MD36htsJk8m2AAA7Bi5gW9NmVL3YNO4sfTqq1yCAgA4INzAdxYvrvuIy1VXSW+9xYRhAMBpuCwF37BapX/8o26vnTzZthqKYAMAqAIjN/CNUaOksjLXXhMdLb34IpehAAA1ItzAu+q6QR8b8gEAnMRlKXhPdrbUqpXrwWbSJDbkAwA4jZEbeEd2tnTlla6/jv1rAAAuYuQGnme1ShMmuP66q64i2AAAXEa4geeNGiXt2+faa5o1sy31BgDARYQbeNaUKbZl2656+WXm2AAA6oRwA8+p6yZ9U6ZIw4a5vx4AQEgg3MAz6rpJ3+TJ0mOPub8eAEDIYLUUPCMz07VN+po0kRYsYIM+AEC9EW7gfosX2/alcVaTJtLvv0vh4Z6rCQAQMrgsBfcqL5fGjnXtNa++SrABALgN4Qbuk50ttWwpHTni/GuYPAwAcDMuS8E96rID8VVXMXkYAOB2jNyg/uqyAzGb9AEAPIRwg/p78EHXdyBmkz4AgIcQblA/Vqvrl5ZGjGCeDQDAYwg3qJ/MTOnQIefbx8VJb7zhuXoAACGPcIO6c3U/G0maP5/LUQAAjyLcoG5cvb1CdLS0dKmUkeG5mgAAEEvBUVcPPuj87RUiI6XffmOjPgCAVzByA9e5Ool4+nSCDQDAawg3cN2DDzo/iTg6Wrr7bs/WAwDAHxBu4BpXR21eeokJxAAAryLcwDWujNqwnw0AwAcIN3CeK6M2TZuynw0AwCcIN3DeP//p/KjNHXdwOQoA4BOEGzhn8WJp5kzn2kZFMYkYAOAz7HOD2mVnS8OHO99+6lRGbQAAPsPIDWpmtUoTJjjfnlEbAICPEW5QswcflPbtc749ozYAAB+zGGOMr4vwprKyMsXExKi0tFTR0dG+Lse/Wa1SbKzzk4hbtJBKSgg3AAC3c+X3NyM3qJ4re9pI0gsvEGwAAD5HuEHVXNnTJizMtpqKO34DAPwA4QZVc2XU5t572YkYAOA3mHOD07ky1yYqSjpwgMtRAACPYs4N6seVURtWRwEA/AwjN3DEqA0AwA8xcoO6Y9QGABDgGLnBfzFqAwDwU4zcoG4YtQEABAHCDWysVmnuXOfacv8oAIAfI9zAZs0aaf9+59oyagMA8GOEG9i8845z7Ri1AQD4OcINbLdOeOop59oyagMA8HMNfV0AfCw7Wxo+3Lm20dGM2gAA/B4jN6HMapUmTHC+/TXXMGoDAPB7hJtQ9uCD0r59zrcfMsRztQAA4CaEm1DlytJvSUpOlgYM8Fw9AAC4CeEmVLmy9FuS5szhkhQAICAQbkLV8uXOtQsLs62mysjwbD0AALgJ4SYUWa3Syy871/bee6VhwzxbDwAAbkS4CUUPPiiVldXeLjraFm4AAAgghJtQ48pEYpZ+AwACEOEm1LgykZil3wCAAES4CTXOTiRu0YKl3wCAgOTzcDNv3jylpKQoMjJS/fr109dff11j+zlz5qhLly5q3LixkpOTdfvtt+vYsWNeqjbAWa3S//2fc21vvZVLUgCAgOTTcLNw4UJNmjRJM2bM0IYNG9SzZ0+lp6drz549VbZ/8803NW3aNM2YMUM//vijXnrpJS1cuFB33XWXlysPUGvWSHv31t6Oe0gBAAKYT8PN7NmzNX78eI0bN07dunXTc889pyZNmujlapYp//vf/1b//v01atQopaSk6OKLL9bIkSNrHO05fvy4ysrKHB4hy9lLUkwkBgAEMJ+Fm/Lycq1fv15paWn/LSYsTGlpacrNza3yNRdccIHWr19vDzNbt27VihUr9Ne//rXaz8nKylJMTIz9kZyc7N6OBApXLkkxkRgAEMAa+uqD9+7dK6vVqoSEBIfjCQkJ+umnn6p8zahRo7R371796U9/kjFGJ0+e1PXXX1/jZanp06dr0qRJ9udlZWWhGXCcvSTVsiUTiQEAAc3nE4pdkZOTo4ceekjPPPOMNmzYoOzsbL3//vt64IEHqn1NRESEoqOjHR4hydlLUpmZXJICAAQ0n43cxMfHq0GDBiopKXE4XlJSosTExCpfc++99+rvf/+7/vGPf0iSunfvrsOHD2vChAm6++67FRYWUFnNe7gkBQAIIT5LA+Hh4erdu7dWrVplP1ZRUaFVq1YpNTW1ytccOXLktADT4D+jDMYYzxUb6LgkBQAIIT4buZGkSZMmacyYMerTp4/69u2rOXPm6PDhwxo3bpwkafTo0WrTpo2ysrIkSZdffrlmz56tc845R/369VNBQYHuvfdeXX755faQgypwSQoAEEJ8Gm5GjBih3377Tffdd5+Ki4vVq1cvrVy50j7JuLCw0GGk5p577pHFYtE999yjnTt3qmXLlrr88sv14IMP+qoL/o9LUgCAEGMxIXY9p6ysTDExMSotLQ2NycWzZkkzZtTermVLafduRm4AAH7Jld/fzMANZtnZzgUbiUtSAICgQbgJVlarNHGi8+25JAUACBKEm2C1Zo20Y4dzbZOTWSUFAAgahJtg5ewKKUmaM4dLUgCAoEG4CUaurJCaOVPKyPBsPQAAeBHhJhg5u2lffLx0992erwcAAC8i3AQjZy9J/e//cjkKABB0CDfBhk37AAAhjnATbLiPFAAgxBFugg33kQIAhDjCTTDhkhQAAISboMIlKQAACDdBZfdu59pxSQoAEMQIN8Fkyxbn2nFJCgAQxAg3wcJqlV54ofZ2bdtySQoAENQIN8FizRpp587a240fzyUpAEBQI9wEC2eXgHfq5Nk6AADwMcJNMHBlCXhSkmdrAQDAxwg3wYAl4AAA2BFuggFLwAEAsCPcBAOWgAMAYEe4CXQsAQcAwAHhJtCxBBwAAAeEm0DHEnAAABwQbgJZdrY0Z45zbVkCDgAIEQ19XQDqyGqVJk6svZ3FwnwbAEBIYeQmUK1ZI+3YUXs7Y2yjO8y3AQCECMJNoHJ2b5vbbpMyMjxaCgAA/oRwE6jY2wYAgCoRbgIRe9sAAFAtwk0gYm8bAACqRbgJRM7Ot2FvGwBACCLcBCJn59uwtw0AIAQRbgIN820AAKgR4SbQMN8GAIAaEW4CDfNtAACoEeEm0DDfBgCAGhFuAgnzbQAAqBXhJpAw3wYAgFoRbgIJ820AAKgV4SaQMN8GAIBaEW4CBfNtAABwCuEmUDDfBgAApxBuAgXzbQAAcArhJlA4O4+G+TYAgBBHuAkUv/1We5vkZObbAABCHuEmEFit0qRJtbebPZv5NgCAkEe4CQRr1kg7dtTeLj7e87UAAODnCDeBwNnJxM62AwAgiBFuAgGTiQEAcFqdws2aNWv0v//7v0pNTdXO/+y98vrrr+uLL75wa3H4DyYTAwDgNJfDzdKlS5Wenq7GjRvr22+/1fHjxyVJpaWleuihh9xeYMhjMjEAAC5xOdz885//1HPPPaf58+erUaNG9uP9+/fXhg0b3FocxGRiAABc5HK4yc/P14UXXnja8ZiYGB04cMAdNeGPmEwMAIBLXA43iYmJKigoOO34F198oQ4dOrilKPwBk4kBAHCJy+Fm/PjxmjhxotauXSuLxaJdu3bpjTfe0JQpU3TDDTd4osbQxmRiAABc0tDVF0ybNk0VFRUaPHiwjhw5ogsvvFARERGaMmWKbrnlFk/UGLqYTAwAgMssxhhTlxeWl5eroKBAhw4dUrdu3RQVFeXu2jyirKxMMTExKi0tVXR0tK/LqVlOjnTRRbW3W71aGjTI09UAAOAzrvz+dnnkprS0VFarVXFxcerWrZv9+P79+9WwYUP/DwyBhMnEAAC4zOU5N1dffbXefvvt044vWrRIV199tVuKwn9s2eJcOyYTAwBg53K4Wbt2rS6q4lLJoEGDtHbtWrcUBUnZ2dKMGTW3sViYTAwAwClcDjfHjx/XyZMnTzt+4sQJHT161C1FhTyrVZo40bm2c+YwmRgAgD9wOdz07dtXL7zwwmnHn3vuOfXu3dstRYU8Z3clvv9+KSPD4+UAABBI6nT7hRdffFEXXnihZs6cqZkzZ+rCCy/Uyy+/XKd7S82bN08pKSmKjIxUv3799PXXX9fY/sCBA7rpppuUlJSkiIgIde7cWStWrHD5c/2asxOEO3XybB0AAAQgl8NN//79lZubq+TkZC1atEjvvfeeOnbsqE2bNmmAi3M/Fi5cqEmTJmnGjBnasGGDevbsqfT0dO3Zs6fK9uXl5frzn/+s7du3a8mSJcrPz9f8+fPVpk0bV7vh39iVGACAOqvzPjfu0K9fP5133nl6+umnJUkVFRVKTk7WLbfcomnTpp3W/rnnntNjjz2mn376yeGmna4IiH1uFi+Whg+vuU1ysrRtG/NtAAAhwaP73Ei2EFJQUKA9e/aooqLC4VxVN9WsSnl5udavX6/p06fbj4WFhSktLU25ublVvubdd99VamqqbrrpJi1fvlwtW7bUqFGjdOedd6pBNb/kjx8/ruPHj9ufl5WVOVWfz7ArMQAA9eJyuPnqq680atQo/frrrzp10MdischqtTr1Pnv37pXValVCQoLD8YSEBP30009Vvmbr1q369NNPlZmZqRUrVqigoEA33nijTpw4oRnVLJvOysrSzJkznarJLzg7mTg+3vO1AAAQgFyec3P99derT58+2rx5s/bv36/ff//d/ti/f78narSrqKhQq1at9MILL6h3794aMWKE7r77bj333HPVvmb69OkqLS21P4qKijxaY72xKzEAAPXi8sjNli1btGTJEnXs2LFeHxwfH68GDRqopKTE4XhJSYkSExOrfE1SUpIaNWrkcAnqrLPOUnFxscrLyxUeHn7aayIiIhQREVGvWr2KycQAANSLyyM3/fr1U0FBQb0/ODw8XL1799aqVavsxyoqKrRq1SqlpqZW+Zr+/furoKDAYZ7Pzz//rKSkpCqDTUD67bfa27ArMQAA1XJ55OaWW27R5MmTVVxcrO7du5+2aqlHjx5Ov9ekSZM0ZswY9enTR3379tWcOXN0+PBhjRs3TpI0evRotWnTRllZWZKkG264QU8//bQmTpyoW265RVu2bNFDDz2kW2+91dVu+CcmEwMAUG8uh5srr7xSknTNNdfYj1ksFhljXJpQLEkjRozQb7/9pvvuu0/FxcXq1auXVq5caZ9kXFhYqLCw/w4uJScn68MPP9Ttt9+uHj16qE2bNpo4caLuvPNOV7vhn5hMDABAvbm8z82vv/5a4/l27drVqyBP8+t9bt56Sxo1qvZ2b74pjRzp+XoAAPATHt3nxt/DS0BjMjEAAPVWp038JOmHH35QYWGhysvLHY5fccUV9S4qZA0YILVoIe3bV/V5i0Vq25bJxAAA1MDlcLN161b97W9/03fffWefayPZ5t1IcmnODU6xfHn1wUaSjJHmzGEyMQAANXB5KfjEiRPVvn177dmzR02aNNH333+vzz//XH369FFOTo4HSgwRVqs0cWLNbVq0kIYM8U49AAAEKJfDTW5urmbNmqX4+HiFhYUpLCxMf/rTn5SVlRU8S7J9wZmVUvv22doBAIBquRxurFarmjVrJsm2y/CuXbsk2SYa5+fnu7e6UMJtFwAAcAuX59ycffbZ2rhxo9q3b69+/frp0UcfVXh4uF544QV16NDBEzWGBlZKAQDgFi6Hm3vuuUeHDx+WJM2aNUuXXXaZBgwYoBYtWmjhwoVuLzBkcNsFAADcwuVN/Kqyf/9+NW/e3L5iyp/55SZ+VquUklL7nJvFi6Vhw7xSEgAA/sSjm/hVJS4uzh1vE7q47QIAAG7jVLjJyMjQggULFB0drYyMjBrbZmdnu6WwkMJkYgAA3MapcBMTE2O/5BQTE+PRgkISk4kBAHAbl+bcGGNUVFSkli1bqnHjxp6sy2P8ds5NQkLtt13Yto3diQEAIcmV398u7XNjjFHHjh21w5n5IXAet10AAMBtXAo3YWFh6tSpk/bV9IsYruG2CwAAuJXLOxQ//PDDmjp1qjZv3uyJekIPt10AAMCtXF4KPnr0aB05ckQ9e/ZUeHj4aXNv9u/f77biQgIrpQAAcCuXw82cOXM8UEYIY6UUAABu5ZYdigOJ362WYqUUAAC18toOxceOHVN5ebnDMb8IDIGElVIAALiVyxOKDx8+rJtvvlmtWrVS06ZN1bx5c4cHXMBKKQAA3M7lcHPHHXfo008/1bPPPquIiAi9+OKLmjlzplq3bq3XXnvNEzUGL1ZKAQDgdi5flnrvvff02muvadCgQRo3bpwGDBigjh07ql27dnrjjTeUmZnpiTqDEyulAABwO5dHbvbv368OHTpIss2vqVz6/ac//Umff/65e6sLdqyUAgDA7VwONx06dNC2bdskSV27dtWiRYsk2UZ0YmNj3Vpc0BswwLYS6j83JT2NxSIlJ9vaAQAAp7gcbsaNG6eNGzdKkqZNm6Z58+YpMjJSt99+u6ZOner2AoNagwbSyJG2FVHVYaUUAAAuqfc+N7/++qvWr1+vjh07qkePHu6qy2P8ap+b7Gxp2LDqw83UqdKjj3q3JgAA/JArv79dDjdFRUVKTk6uV4G+5DfhxmqVUlJqXi2VnMzmfQAAyLXf3y5flkpJSdHAgQM1f/58/f7773UuMuQ5swy8qIhl4AAAuMjlcLNu3Tr17dtXs2bNUlJSkoYOHaolS5bo+PHjnqgveLEMHAAAj3A53Jxzzjl67LHHVFhYqA8++EAtW7bUhAkTlJCQoGuuucYTNQYnloEDAOARbrlx5oYNG3Tttddq06ZNslqt7qjLYwJmzg03zAQAwM6jc24q7dixQ48++qh69eqlvn37KioqSvPmzavr24We5culo0erPle57w3LwAEAcJnLt194/vnn9eabb+rLL79U165dlZmZqeXLl6tdu3aeqC841bYEPC5OeuEFKSPDu3UBABAEXL4slZycrJEjRyozM1M9e/b0VF0e4/PLUs4sAW/bVtq+nVEbAAD+w5Xf3y6P3BQWFspS3e0CUDtnloDv2GFrN2iQV0oCACCYuDznhmBTTywBBwDAo+o8oRh1xBJwAAA8inDjbdwJHAAAjyLceFuDBtLcuVWfYwk4AAD15vKE4j/au3ev1q5dK6vVqvPOO09JXEpxXlyctG/f6cdYAg4AQL3UOdwsXbpU1157rTp37qwTJ04oPz9f8+bN07hx49xZX/CpaY+bU8MOAABwmdP73Bw6dEhRUVH25z169NCSJUvUuXNnSdL777+v8ePHa9euXZ6p1E18us8Nt1wAAKBOPHL7hd69e2v58uX25w0bNtSePXvsz0tKShQeHl6HckNIbXvcGCMVFdnaAQCAOnH6stSHH36om266SQsWLNC8efM0d+5cjRgxQlarVSdPnlRYWJgWLFjgwVKDAHvcAADgcU6Hm5SUFL3//vt66623NHDgQN16660qKChQQUGBrFarunbtqsjISE/WGvjY4wYAAI9zeSn4yJEj9c0332jjxo0aNGiQKioq1KtXL4KNM9jjBgAAj3Mp3KxYsUJPPPGE1q1bpxdffFGPPvqoMjMzNXXqVB09etRTNQYP9rgBAMDjnA43kydP1rhx4/TNN9/ouuuu0wMPPKCBAwdqw4YNioyM1DnnnKMPPvjAk7UGj7i4qo8tWcIeNwAA1JPTS8FbtGihjz76SL1799b+/ft1/vnn6+eff7af/+GHH3TddddpjZ+v9PHpUvCa9riRpKVLCTcAAFTBI0vBmzZtqm3btkmSioqKTptj061bN78PNj5ltUoTJ1YfbCwW6bbbbO0AAECdOR1usrKyNHr0aLVu3VoDBw7UAw884Mm6gg973AAA4BVOLwXPzMzUJZdcoq1bt6pTp06KjY31YFlBiD1uAADwCpfuLdWiRQu1aNHCU7UEN/a4AQDAK1ze5wZ1xB43AAB4BeHGW9jjBgAAryDceFNGhrRokdSkiePxtm3Z4wYAADch3HhTdrZ0++3SkSP/PRYfLz3xBMEGAAA3cWlCMeqhug389u2TRoywXY4i4AAAUG+M3HhDTRv4VR5jAz8AANyCcOMNbOAHAIDXEG68gQ38AADwGsKNN7CBHwAAXuMX4WbevHlKSUlRZGSk+vXrp6+//tqp17399tuyWCwaOnSoZwusLzbwAwDAa3webhYuXKhJkyZpxowZ2rBhg3r27Kn09HTt2bOnxtdt375dU6ZM0YBACARs4AcAgNf4PNzMnj1b48eP17hx49StWzc999xzatKkiV5++eVqX2O1WpWZmamZM2eqQ4cOXqy2nuLiqj7GBn4AALiNT8NNeXm51q9fr7S0NPuxsLAwpaWlKTc3t9rXzZo1S61atdK1115b62ccP35cZWVlDg+vq9zjZt++089VdQwAANSZT8PN3r17ZbValZCQ4HA8ISFBxcXFVb7miy++0EsvvaT58+c79RlZWVmKiYmxP5KTk+tdt0tq2uNGsl2WYo8bAADcxueXpVxx8OBB/f3vf9f8+fMVHx/v1GumT5+u0tJS+6OoqMjDVZ6CPW4AAPAqn95+IT4+Xg0aNFBJSYnD8ZKSEiUmJp7W/pdfftH27dt1+eWX249VVFRIkho2bKj8/HydeeaZDq+JiIhQRESEB6p3EnvcAADgVT4duQkPD1fv3r21atUq+7GKigqtWrVKqampp7Xv2rWrvvvuO+Xl5dkfV1xxhS666CLl5eV5/5KTM9jjBgAAr/L5jTMnTZqkMWPGqE+fPurbt6/mzJmjw4cPa9y4cZKk0aNHq02bNsrKylJkZKTOPvtsh9fHxsZK0mnH/UblHjc7d1Y978ZisZ0PhCXtAAAEAJ+HmxEjRui3337Tfffdp+LiYvXq1UsrV660TzIuLCxUWFhATQ1yVLnHzbBhp59jjxsAANzOYkx1y3iCU1lZmWJiYlRaWqro6GjvffCSJdKoUdKJE/89lpxsCzbscQMAQI1c+f0dwEMiASQ7W7r9dsdgEx8vPfEEwQYAADfz+WWpoFe5gd+pA2T79kkjRtguRxFwAABwG0ZuPKmmDfwqj7GBHwAAbkW48SQ28AMAwOsIN57EBn4AAHgd4caT2MAPAACvI9x4UuUGfpX72ZzKYrEtB2cDPwAA3IZw40mVG/hVhQ38AADwCMKNp2Vk2Dbwa9TI8XjbtrbjLAMHAMCt2OfGG4YMkSIibJv4TZggDR8uDRrEiA0AAB7AyI2nZWdL7dpJhw7Znr/wgjR2rLR8uU/LAgAgWBFuPKlyd+KdOx2P79xpO56d7Zu6AAAIYoQbT2F3YgAAfIJw4ynsTgwAgE8QbjyF3YkBAPAJwo2nsDsxAAA+QbjxFHYnBgDAJwg3nsLuxAAA+AThxpMqdycOO+Wvmd2JAQDwGHYo9rRLL5UqKmw/P/+81Lmz7VIUIzYAAHgE4caTrNb/btTXqJF0zTVSQ/7KAQDwJC5LeUp2tpSSIo0aZXt+4oTUvj27EgMA4GGEG0+ovO3CqZv4cdsFAAA8jnDjbtx2AQAAnyLcuBu3XQAAwKcIN+7GbRcAAPApwo27cdsFAAB8inDjbtx2AQAAnyLcuNsfb7twasDhtgsAAHgc4cYTKm+7cOqlJ267AACAx7FdrqdkZEidOkk9ekhNmkjvv89tFwAA8AJGbjzFapU++sj2c3w8wQYAAC8h3HhC5a0XpkyxPS8stD1nZ2IAADyOcONu3HoBAACfIty4E7deAADA5wg37sStFwAA8DnCjTtx6wUAAHyOcONO3HoBAACfI9y4E7deAADA5wg37sStFwAA8DnCjbtV3nohIcHxOLdeAADAK7j9gidkZEgxMVJampSYKL31FjsUAwDgJYzceILVKuXk2H5OSCDYAADgRYQbd6u89cI//2l7vnEjt14AAMCLCDfuxK0XAADwOcKNu3DrBQAA/ALhxl249QIAAH6BcOMu3HoBAAC/QLhxF269AACAXyDcuAu3XgAAwC8QbtyFWy8AAOAXCDfuxK0XAADwOW6/4G4ZGbZw86c/SfHx0uLF7FAMAIAXEW484fffbX+2aycNGuTTUgAACDVclvKEfftsf8bH+7YOAABCEOHGEyrDTYsWvq0DAIAQRLjxhL17bX8ycgMAgNcRbtzNapW++872c1kZ95ICAMDLCDfulJ0tpaRI//qX7fmCBbbn3A0cAACvIdy4S3a2NGzY6TfP3LnTdpyAAwCAVxBu3MFqlSZOtN35+1SVx267jUtUAAB4AeHGHdasOX3E5o+MkYqKbO0AAIBHEW7cYfdu97YDAAB1Rrhxh6Qk97YDAAB15hfhZt68eUpJSVFkZKT69eunr7/+utq28+fP14ABA9S8eXM1b95caWlpNbb3igEDbDfHPPVu4JUsFik52dYOAAB4lM/DzcKFCzVp0iTNmDFDGzZsUM+ePZWenq49e/ZU2T4nJ0cjR47U6tWrlZubq+TkZF188cXauXOnlyv/gwYNpLlzbT+fGnAqn8+Zw80zAQDwAosxVS3x8Z5+/frpvPPO09NPPy1JqqioUHJysm655RZNmzat1tdbrVY1b95cTz/9tEaPHn3a+ePHj+v48eP252VlZUpOTlZpaamio6Pd1xHJttz7hhukPwaz5GRbsMnIcO9nAQAQQsrKyhQTE+PU72+fjtyUl5dr/fr1SktLsx8LCwtTWlqacnNznXqPI0eO6MSJE4qLi6vyfFZWlmJiYuyP5ORkt9RepYwM6f/+z/Zz27bS6tXStm0EGwAAvMin4Wbv3r2yWq1KSEhwOJ6QkKDi4mKn3uPOO+9U69atHQLSH02fPl2lpaX2R1FRUb3rrtGhQ7Y/k5OlQYO4FAUAgJc19HUB9fHwww/r7bffVk5OjiIjI6tsExERoYiICO8VVVZm+zMmxnufCQAA7HwabuLj49WgQQOVlJQ4HC8pKVFiYmKNr3388cf18MMP65NPPlGPHj08WaZrSkttf7p7Pg8AAHCKTy9LhYeHq3fv3lq1apX9WEVFhVatWqXU1NRqX/foo4/qgQce0MqVK9WnTx9vlOo8Rm4AAPApn1+WmjRpksaMGaM+ffqob9++mjNnjg4fPqxx48ZJkkaPHq02bdooKytLkvTII4/ovvvu05tvvqmUlBT73JyoqChFRUX5rB92jNwAAOBTPg83I0aM0G+//ab77rtPxcXF6tWrl1auXGmfZFxYWKiwsP8OMD377LMqLy/XsGHDHN5nxowZuv/++71ZetUYuQEAwKd8Hm4k6eabb9bNN99c5bmcnByH59u3b/d8QfXByA0AAD7l8x2Kgw4jNwAA+BThxt0qR24INwAA+AThxp2sVmnXLtvPW7fangMAAK8i3LhLdraUkiJVzgmaMsX2PDvbh0UBABB6CDfukJ0tDRsm7djheHznTttxAg4AAF5DuKkvq1WaOFGq6ubqlcduu41LVAAAeAnhpr7WrDl9xOaPjJGKimztAACAxxFu6mv3bve2AwAA9UK4qa+kJPe2AwAA9UK4qa8BA6S2bSWLperzFouUnGxrBwAAPI5wU18NGkhz51Z9rjLwzJljawcAADyOcOMOGRnSkiVSixaOx9u2tR3PyPBNXQAAhCC/uHFmUMjIkI4dkzIzpW7dpHnzbJeiGLEBAMCrCDfudOSI7c+OHaVBg3xaCgAAoYrLUu506JDtz6go39YBAEAII9y408GDtj8JNwAA+Azhxp0YuQEAwOcIN+5EuAEAwOcIN+5UGW6aNfNtHQAAhDDCjTsxcgMAgM8RbtyJcAMAgM8RbtyJ1VIAAPgc4cadGLkBAMDnCDfuxIRiAAB8jnDjTozcAADgc4QbdyLcAADgc4Qbdzl+XDp61PZzXp5ktfq0HAAAQhXhxh2ys6UOHf77fOhQKSXFdhwAAHgV4aa+srOlYcOkXbscj+/caTtOwAEAwKsIN/VhtUoTJ0rGnH6u8thtt3GJCgAALyLc1MeaNdKOHdWfN0YqKrK1AwAAXkG4qY/du93bDgAA1Bvhpj6SktzbDgAA1Bvhpj4GDJDatpUslqrPWyxScrKtHQAA8ArCTX00aCDNnWv7+dSAU/l8zhxbOwAA4BWEm/rKyJCWLJHatHE83rat7XhGhm/qAgAgRDX0dQFBISNDGjLEtipq927bHJsBAxixAQDABwg37tKggTRokK+rAAAg5HFZCgAABBXCDQAACCqEGwAAEFQINwAAIKgQbgAAQFAh3AAAgKBCuAEAAEGFcAMAAIIK4QYAAASVkNuh2BgjSSorK/NxJQAAwFmVv7crf4/XJOTCzcGDByVJycnJPq4EAAC46uDBg4qJiamxjcU4E4GCSEVFhXbt2qVmzZrJYrG47X3LysqUnJysoqIiRUdHu+19/UWw908K/j4Ge/+k4O9jsPdPCv4+Bnv/JM/10RijgwcPqnXr1goLq3lWTciN3ISFhalt27Yee//o6Oig/QcrBX//pODvY7D3Twr+PgZ7/6Tg72Ow90/yTB9rG7GpxIRiAAAQVAg3AAAgqBBu3CQiIkIzZsxQRESEr0vxiGDvnxT8fQz2/knB38dg758U/H0M9v5J/tHHkJtQDAAAghsjNwAAIKgQbgAAQFAh3AAAgKBCuAEAAEGFcOMG8+bNU0pKiiIjI9WvXz99/fXXvi6pzu6//35ZLBaHR9euXe3njx07pptuukktWrRQVFSUrrzySpWUlPiw4pp9/vnnuvzyy9W6dWtZLBa98847DueNMbrvvvuUlJSkxo0bKy0tTVu2bHFos3//fmVmZio6OlqxsbG69tprdejQIS/2oma19XHs2LGnfaeXXHKJQxt/7mNWVpbOO+88NWvWTK1atdLQoUOVn5/v0MaZf5eFhYW69NJL1aRJE7Vq1UpTp07VyZMnvdmVKjnTv0GDBp32HV5//fUObfy1f5L07LPPqkePHvZN3VJTU/XBBx/Yzwfy9yfV3r9A//5O9fDDD8tisei2226zH/O779CgXt5++20THh5uXn75ZfP999+b8ePHm9jYWFNSUuLr0upkxowZ5v/9v/9ndu/ebX/89ttv9vPXX3+9SU5ONqtWrTLr1q0z559/vrngggt8WHHNVqxYYe6++26TnZ1tJJlly5Y5nH/44YdNTEyMeeedd8zGjRvNFVdcYdq3b2+OHj1qb3PJJZeYnj17mq+++sqsWbPGdOzY0YwcOdLLPalebX0cM2aMueSSSxy+0/379zu08ec+pqenm1deecVs3rzZ5OXlmb/+9a/mjDPOMIcOHbK3qe3f5cmTJ83ZZ59t0tLSzLfffmtWrFhh4uPjzfTp033RJQfO9G/gwIFm/PjxDt9haWmp/bw/988YY959913z/vvvm59//tnk5+ebu+66yzRq1Mhs3rzZGBPY358xtfcv0L+/P/r6669NSkqK6dGjh5k4caL9uL99h4Sbeurbt6+56aab7M+tVqtp3bq1ycrK8mFVdTdjxgzTs2fPKs8dOHDANGrUyCxevNh+7McffzSSTG5urpcqrLtTf/FXVFSYxMRE89hjj9mPHThwwERERJi33nrLGGPMDz/8YCSZb775xt7mgw8+MBaLxezcudNrtTurunAzZMiQal8TaH3cs2ePkWQ+++wzY4xz/y5XrFhhwsLCTHFxsb3Ns88+a6Kjo83x48e924FanNo/Y2y/HP/4i+RUgdS/Ss2bNzcvvvhi0H1/lSr7Z0zwfH8HDx40nTp1Mh9//LFDn/zxO+SyVD2Ul5dr/fr1SktLsx8LCwtTWlqacnNzfVhZ/WzZskWtW7dWhw4dlJmZqcLCQknS+vXrdeLECYf+du3aVWeccUZA9nfbtm0qLi526E9MTIz69etn709ubq5iY2PVp08fe5u0tDSFhYVp7dq1Xq+5rnJyctSqVSt16dJFN9xwg/bt22c/F2h9LC0tlSTFxcVJcu7fZW5urrp3766EhAR7m/T0dJWVlen777/3YvW1O7V/ld544w3Fx8fr7LPP1vTp03XkyBH7uUDqn9Vq1dtvv63Dhw8rNTU16L6/U/tXKRi+v5tuukmXXnqpw3cl+ed/gyF340x32rt3r6xWq8OXJUkJCQn66aeffFRV/fTr108LFixQly5dtHv3bs2cOVMDBgzQ5s2bVVxcrPDwcMXGxjq8JiEhQcXFxb4puB4qa67q+6s8V1xcrFatWjmcb9iwoeLi4gKmz5dccokyMjLUvn17/fLLL7rrrrv0l7/8Rbm5uWrQoEFA9bGiokK33Xab+vfvr7PPPluSnPp3WVxcXOX3XHnOX1TVP0kaNWqU2rVrp9atW2vTpk268847lZ+fr+zsbEmB0b/vvvtOqampOnbsmKKiorRs2TJ169ZNeXl5QfH9Vdc/KTi+v7ffflsbNmzQN998c9o5f/xvkHADB3/5y1/sP/fo0UP9+vVTu3bttGjRIjVu3NiHlaGurr76avvP3bt3V48ePXTmmWcqJydHgwcP9mFlrrvpppu0efNmffHFF74uxSOq69+ECRPsP3fv3l1JSUkaPHiwfvnlF5155pneLrNOunTpory8PJWWlmrJkiUaM2aMPvvsM1+X5TbV9a9bt24B//0VFRVp4sSJ+vjjjxUZGenrcpzCZal6iI+PV4MGDU6bEV5SUqLExEQfVeVesbGx6ty5swoKCpSYmKjy8nIdOHDAoU2g9rey5pq+v8TERO3Zs8fh/MmTJ7V///6A7LMkdejQQfHx8SooKJAUOH28+eab9a9//UurV69W27Zt7ced+XeZmJhY5fdcec4fVNe/qvTr10+SHL5Df+9feHi4OnbsqN69eysrK0s9e/bU3Llzg+b7q65/VQm072/9+vXas2ePzj33XDVs2FANGzbUZ599pieffFINGzZUQkKC332HhJt6CA8PV+/evbVq1Sr7sYqKCq1atcrhWmsgO3TokH755RclJSWpd+/eatSokUN/8/PzVVhYGJD9bd++vRITEx36U1ZWprVr19r7k5qaqgMHDmj9+vX2Np9++qkqKirs/4MKNDt27NC+ffuUlJQkyf/7aIzRzTffrGXLlunTTz9V+/btHc478+8yNTVV3333nUOI+/jjjxUdHW2/dOArtfWvKnl5eZLk8B36a/+qU1FRoePHjwf891edyv5VJdC+v8GDB+u7775TXl6e/dGnTx9lZmbaf/a779DtU5RDzNtvv20iIiLMggULzA8//GAmTJhgYmNjHWaEB5LJkyebnJwcs23bNvPll1+atLQ0Ex8fb/bs2WOMsS33O+OMM8ynn35q1q1bZ1JTU01qaqqPq67ewYMHzbfffmu+/fZbI8nMnj3bfPvtt+bXX381xtiWgsfGxprly5ebTZs2mSFDhlS5FPycc84xa9euNV988YXp1KmT3yyTNqbmPh48eNBMmTLF5Obmmm3btplPPvnEnHvuuaZTp07m2LFj9vfw5z7ecMMNJiYmxuTk5DgspT1y5Ii9TW3/LiuXoV588cUmLy/PrFy50rRs2dIvltrW1r+CggIza9Yss27dOrNt2zazfPly06FDB3PhhRfa38Of+2eMMdOmTTOfffaZ2bZtm9m0aZOZNm2asVgs5qOPPjLGBPb3Z0zN/QuG768qp64A87fvkHDjBk899ZQ544wzTHh4uOnbt6/56quvfF1SnY0YMcIkJSWZ8PBw06ZNGzNixAhTUFBgP3/06FFz4403mubNm5smTZqYv/3tb2b37t0+rLhmq1evNpJOe4wZM8YYY1sOfu+995qEhAQTERFhBg8ebPLz8x3eY9++fWbkyJEmKirKREdHm3HjxpmDBw/6oDdVq6mPR44cMRdffLFp2bKladSokWnXrp0ZP378aeHbn/tYVd8kmVdeecXexpl/l9u3bzd/+ctfTOPGjU18fLyZPHmyOXHihJd7c7ra+ldYWGguvPBCExcXZyIiIkzHjh3N1KlTHfZJMcZ/+2eMMddcc41p166dCQ8PNy1btjSDBw+2BxtjAvv7M6bm/gXD91eVU8ONv32HFmOMcf94EAAAgG8w5wYAAAQVwg0AAAgqhBsAABBUCDcAACCoEG4AAEBQIdwAAICgQrgBAABBhXADAACCCuEGwGkWLFig2NhYX5cBAHVCuAFCyNixY2WxWPTwww87HH/nnXdksVh8VFVoGDt2rIYOHerrMoCQQLgBQkxkZKQeeeQR/f77774uBQA8gnADhJi0tDQlJiYqKyur1rbvvPOOOnXqpMjISKWnp6uoqKjG9jt27NDIkSMVFxenpk2bqk+fPlq7dq39/LPPPqszzzxT4eHh6tKli15//XWH11ssFj3//PO67LLL1KRJE5111lnKzc1VQUGBBg0apKZNm+qCCy7QL7/8Yn/N/fffr169eun5559XcnKymjRpouHDh6u0tNTepqKiQrNmzVLbtm0VERGhXr16aeXKlfbz5eXluvnmm5WUlKTIyEi1a9fO4e9n9uzZ6t69u5o2bark5GTdeOONOnTokP185WW8Dz/8UGeddZaioqJ0ySWXaPfu3fYaX331VS1fvlwWi0UWi0U5OTmSpKKiIg0fPlyxsbGKi4vTkCFDtH37dvt75+TkqG/fvmratKliY2PVv39//frrr7V+d0AoI9wAIaZBgwZ66KGH9NRTT2nHjh3Vtjty5IgefPBBvfbaa/ryyy914MABXX311dW2P3TokAYOHKidO3fq3Xff1caNG3XHHXeooqJCkrRs2TJNnDhRkydP1ubNm3Xddddp3LhxWr16tcP7PPDAAxo9erTy8vLUtWtXjRo1Stddd52mT5+udevWyRijm2++2eE1BQUFWrRokd577z2tXLlS3377rW688Ub7+blz5+qJJ57Q448/rk2bNik9PV1XXHGFtmzZIkl68skn9e6772rRokXKz8/XG2+8oZSUFPvrw8LC9OSTT+r777/Xq6++qk8//VR33HHHaX9fjz/+uF5//XV9/vnnKiws1JQpUyRJU6ZM0fDhw+2BZ/fu3brgggt04sQJpaenq1mzZlqzZo2+/PJLezAqLy/XyZMnNXToUA0cOFCbNm1Sbm6uJkyYwCVEoDYeudc4AL80ZswYM2TIEGOMMeeff7655pprjDHGLFu2zPzxfwevvPKKkWS++uor+7Eff/zRSDJr166t8r2ff/5506xZM7Nv374qz19wwQVm/PjxDseuuuoq89e//tX+XJK555577M9zc3ONJPPSSy/Zj7311lsmMjLS/nzGjBmmQYMGZseOHfZjH3zwgQkLCzO7d+82xhjTunVr8+CDDzp89nnnnWduvPFGY4wxt9xyi/mf//kfU1FRUWXtp1q8eLFp0aKF/Xnl31dBQYH92Lx580xCQoL9+R//7iu9/vrrpkuXLg6fe/z4cdO4cWPz4Ycfmn379hlJJicnx6m6ANgwcgOEqEceeUSvvvqqfvzxxyrPN2zYUOedd579edeuXRUbG1tt+7y8PJ1zzjmKi4ur8vyPP/6o/v37Oxzr37//ae/Xo0cP+88JCQmSpO7duzscO3bsmMrKyuzHzjjjDLVp08b+PDU1VRUVFcrPz1dZWZl27dpV42ePHTtWeXl56tKli2699VZ99NFHDm0/+eQTDR48WG3atFGzZs3097//Xfv27dORI0fsbZo0aaIzzzzT/jwpKUl79uyp8u+i0saNG1VQUKBmzZopKipKUVFRiouL07Fjx/TLL78oLi5OY8eOVXp6ui6//HLNnTvXfqkLQPUIN0CIuvDCC5Wenq7p06e75f0aN27slvdp1KiR/efKyy9VHau83OUO5557rrZt26YHHnhAR48e1fDhwzVs2DBJ0vbt23XZZZepR48eWrp0qdavX6958+ZJss3VqaruyjqNMTV+7qFDh9S7d2/l5eU5PH7++WeNGjVKkvTKK68oNzdXF1xwgRYuXKjOnTvrq6++clvfgWBEuAFC2MMPP6z33ntPubm5p507efKk1q1bZ3+en5+vAwcO6KyzzqryvXr06KG8vDzt37+/yvNnnXWWvvzyS4djX375pbp161aPHtgUFhZq165d9udfffWVwsLC1KVLF0VHR6t169a1fnZ0dLRGjBih+fPna+HChVq6dKn279+v9evXq6KiQk888YTOP/98de7c2eGznBUeHi6r1epw7Nxzz9WWLVvUqlUrdezY0eERExNjb3fOOedo+vTp+ve//62zzz5bb775psufD4QSwg0Qwrp3767MzEw9+eSTp51r1KiRbrnlFq1du1br16/X2LFjdf7556tv375VvtfIkSOVmJiooUOH6ssvv9TWrVu1dOlSe3CaOnWqFixYoGeffVZbtmzR7NmzlZ2dbZ90Wx+RkZEaM2aMNm7cqDVr1ujWW2/V8OHDlZiYaP/sRx55RAsXLlR+fr6mTZumvLw8TZw4UZJtNdRbb72ln376ST///LMWL16sxMRExcbGqmPHjjpx4oSeeuopbd26Va+//rqee+45l2tMSUnRpk2blJ+fr7179+rEiRPKzMxUfHy8hgwZojVr1mjbtm3KycnRrbfeqh07dmjbtm2aPn26cnNz9euvv+qjjz7Sli1bqg2YAP7D15N+AHhPVZNat23bZsLDw0+bUBwTE2OWLl1qOnToYCIiIkxaWpr59ddfa3z/7du3myuvvNJER0ebJk2amD59+jhMQH7mmWdMhw4dTKNGjUznzp3Na6+95vB6SWbZsmUOtUky3377rf3Y6tWrjSTz+++/G2NsE4p79uxpnnnmGdO6dWsTGRlphg0bZvbv329/jdVqNffff79p06aNadSokenZs6f54IMP7OdfeOEF06tXL9O0aVMTHR1tBg8ebDZs2GA/P3v2bJOUlGQaN25s0tPTzWuvveZQQ+Xf1x+dOkl7z5495s9//rOJiooykszq1auNMcbs3r3bjB492sTHx5uIiAjToUMHM378eFNaWmqKi4vN0KFDTVJSkgkPDzft2rUz9913n7FarTV+D0CosxhTy0VhAPBj999/v9555x3l5eX5uhQAfoLLUgAAIKgQbgAAQFDhshQAAAgqjNwAAICgQrgBAABBhXADAACCCuEGAAAEFcINAAAIKoQbAAAQVAg3AAAgqBBuAABAUPn/koRWzEINSKUAAAAASUVORK5CYII=
"
     },
     "metadata": {
      "image/png": {}
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Nombre de composante expliquant 95% de la variance\n",
    "n_components = recherche_nb_composante(df_pca)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nYXra47fjHKpvfMvWwHcQM",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Réduction de dimension PCA\n",
    "# Entrainement de l'algorithme\n",
    "pca = PCA(k=n_components, inputCol='features_scaled', outputCol='vectors_pca')\n",
    "model_pca = pca.fit(df_pca)\n",
    "\n",
    "# Transformation des images sur les k premières composantes\n",
    "df_reduit = model_pca.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "Oq6jluRKpUFFmVpN9ZRpr5",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                path|               label|            features|    features_vectors|     features_scaled|         vectors_pca|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|file:/data/notebo...|          pears2.jpg|[0.0012995078, 0....|[0.00129950779955...|[-0.7444391755076...|[8.50963413230746...|\n",
      "|file:/data/notebo...|           dates.jpg|[0.0010436894, 0....|[0.00104368943721...|[-0.7448472349663...|[-9.0422591224196...|\n",
      "|file:/data/notebo...|      raspberry2.jpg|[0.16309968, 1.03...|[0.16309967637062...|[-0.4863494567430...|[15.8344365895738...|\n",
      "|file:/data/notebo...|       chestnut1.jpg|[0.0014374888, 0....|[0.00143748882692...|[-0.7442190800352...|[-13.592577862649...|\n",
      "|file:/data/notebo...|   cherries_wax7.jpg|[0.25825173, 0.40...|[0.25825172662734...|[-0.3345710865007...|[-7.6162145965254...|\n",
      "|file:/data/notebo...|         apples4.jpg|[2.051978, 2.6198...|[2.05197811126709...|[2.52662703363045...|[15.3057575266027...|\n",
      "|file:/data/notebo...|    apples_pears.jpg|[0.0, 1.4148519, ...|[0.0,1.4148519039...|[-0.7465120386294...|[-7.7387513015271...|\n",
      "|file:/data/notebo...|    raspberries3.jpg|[0.16309968, 1.03...|[0.16309967637062...|[-0.4863494567430...|[15.8344365895738...|\n",
      "|file:/data/notebo...|apple_apricot_nec...|[0.56099874, 0.01...|[0.56099873781204...|[0.14834491824485...|[-9.9283991038218...|\n",
      "|file:/data/notebo...|   cherries_wax1.jpg|[0.60749894, 0.18...|[0.60749894380569...|[0.22251804917166...|[-8.7098499338870...|\n",
      "|file:/data/notebo...|     plums_pears.jpg|[0.8002806, 0.669...|[0.80028057098388...|[0.53002672719607...|[-5.8534578427060...|\n",
      "|file:/data/notebo...| apples_peaches2.jpg|[0.0, 1.5745357E-...|[0.0,1.5745357086...|[-0.7465120386294...|[-9.3544816152452...|\n",
      "|file:/data/notebo...|     pomegranate.jpg|[1.1276621, 0.0, ...|[1.12766206264495...|[1.05223753734398...|[-10.484069110461...|\n",
      "|file:/data/notebo...|       cherries4.jpg|[0.08947858, 0.80...|[0.08947858214378...|[-0.6037834969665...|[20.5308296561306...|\n",
      "|file:/data/notebo...|cherries(rainier)...|[1.3608732, 1.830...|[1.36087322235107...|[1.42423592988520...|[13.3845957840729...|\n",
      "|file:/data/notebo...|   cherries_wax5.jpg|[0.0, 0.0, 0.0, 0...|[0.0,0.0,0.0,0.0,...|[-0.7465120386294...|[14.8488723156880...|\n",
      "|file:/data/notebo...|    pomegranate2.jpg|[3.678786, 0.0, 0...|[3.67878603935241...|[5.12157118203083...|[-9.4829781905890...|\n",
      "|file:/data/notebo...|       cherries8.jpg|[0.015279731, 0.8...|[0.01527973078191...|[-0.7221391257014...|[15.6245446602267...|\n",
      "|file:/data/notebo...|apple_apricot_pea...|[0.99363333, 0.0,...|[0.99363332986831...|[0.83844642541745...|[-10.184132129134...|\n",
      "|file:/data/notebo...|grape_pear_mandar...|[0.21920842, 0.57...|[0.21920841932296...|[-0.3968496137162...|[-8.2285229921931...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Visualisation du dataframe réduit\n",
    "df_reduit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "VWr9OWtfWuP6sdNN6sG9bz",
     "report_properties": {
      "rowId": "0XpyL8VBE0t5t9RbOx2l1t"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/notebook_files/Results\n"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "v73dBQgNesmFxIjiDUaDRL",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/notebook_files/Results2\n"
     ]
    }
   ],
   "source": [
    "#Sauvegarde des données\n",
    "#Finalement, on sauvegarde les données pré-traitées et réduites au format parquet.\n",
    "print(PATH_Result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "SGLasQEDjMexrEpV5d4UoY",
     "report_properties": {
      "rowId": "HIrYuHKyNkVyC0bRttZebP"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Enregistrement des données traitées au format \"**parquet**\"</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "MbZbYHoFyJ7E0nuAlVdAFH",
     "report_properties": {
      "rowId": "yj0PiVYrDLxpBbkzWnSphl"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "DZjvKmnAUthmIBWhXTZfTD",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "# Sauvegarde des données - AFTER PCA\n",
    "df_reduit.write.mode(\"overwrite\").parquet(PATH_Result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "h12BratjPvqnj4ExZ7g0hQ",
     "report_properties": {
      "rowId": "mWjivhVKhHgKd5ZK3CAnWt"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 3.8 Chargement des données enregistrées et validation du résultat\n",
    "\n",
    "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ndU0wr73dKMqi7p8xDi9Yp",
     "report_properties": {
      "rowId": "HsXJNy67COMfybcJmqANGs"
     },
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "GY6ax4OCipKh5D5sngpIMl",
     "type": "CODE"
    }
   },
   "outputs": [],
   "source": [
    "dfPCA = pd.read_parquet(PATH_Result2,engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "iLWosnhqYeAvOxwQJnNVhB",
     "report_properties": {
      "rowId": "b0zghCC4Pbfqwb2r3YQjF1"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>On affiche les 5 premières lignes du DataFrame</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "kFpwPCi5wklbF3lWdZqHTK",
     "report_properties": {
      "rowId": "B9MEriWMFBZRwwQtWv8EJE"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>pears2.jpg</td>\n",
       "      <td>[0.0012995078, 0.02938582, 0.032186225, 0.0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>dates.jpg</td>\n",
       "      <td>[0.0010436894, 0.25667804, 1.3375239, 0.000398...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>raspberry2.jpg</td>\n",
       "      <td>[0.16309968, 1.0358377, 0.0, 0.0, 0.0, 0.89195...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>chestnut1.jpg</td>\n",
       "      <td>[0.0014374888, 0.075059734, 0.0, 0.0, 0.0, 0.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>cherries_wax7.jpg</td>\n",
       "      <td>[0.25825173, 0.40794894, 0.017071003, 0.550319...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "5SCH38pYtIIcDZV7MjXmHY",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>label</th>\n",
       "      <th>features</th>\n",
       "      <th>features_vectors</th>\n",
       "      <th>features_scaled</th>\n",
       "      <th>vectors_pca</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>pears2.jpg</td>\n",
       "      <td>[0.0012995078, 0.02938582, 0.032186225, 0.0, 0...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>dates.jpg</td>\n",
       "      <td>[0.0010436894, 0.25667804, 1.3375239, 0.000398...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>raspberry2.jpg</td>\n",
       "      <td>[0.16309968, 1.0358377, 0.0, 0.0, 0.0, 0.89195...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>chestnut1.jpg</td>\n",
       "      <td>[0.0014374888, 0.075059734, 0.0, 0.0, 0.0, 0.1...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>file:/data/notebook_files/test-multiple_fruits...</td>\n",
       "      <td>cherries_wax7.jpg</td>\n",
       "      <td>[0.25825173, 0.40794894, 0.017071003, 0.550319...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "      <td>{'type': 1, 'size': None, 'indices': None, 'va...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfPCA.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "FIwo5dDc9IwCbIaQhWbMyA",
     "report_properties": {
      "rowId": "i4E1cr7pOM8HSSNIL4Ya39"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>On valide que la dimension du vecteur de caractéristiques des images est bien de dimension 1280</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "E11qnxBKhGYLCxL4pMDeel",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "1MLBy71gDYWOmE3m0pRPFi",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfPCA.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "lHdWJvVN3Mk1ASQ3kJcdem",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>array([0.00129951, 0.02938582, 0.03218623, ..., 0.46300986, 0.02563216,\n",
       "       0.00794988], dtype=float32)</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[0,'features']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "TzMuyKrJ6RvidKwyk4NG0A",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>array([0.00129951, 0.02938582, 0.03218623, ..., 0.46300986, 0.02563216,\n",
       "       0.00794988], dtype=float32)</pre>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfPCA.loc[0,\"features\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "dw8KgJEV06XFX8MEHLWT4A",
     "report_properties": {
      "rowId": "XIdOAnxk9aXRxsM4uEnIIR"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "eHvpRQQiJSNCiKq3wAmJaj",
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1280,)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfPCA.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5nHsD6MgdPTQD3Hgh121H2",
     "report_properties": {
      "rowId": "QYPrBpDvlf1xmq0DqaArM9"
     },
     "type": "MD"
    }
   },
   "source": [
    "Nous venons de valider le processus sur un jeu de données allégé en local <br />\n",
    "où nous avons simulé un cluster de machines en répartissant la charge de travail <br />\n",
    "sur différents cœurs de processeur au sein d'une même machine.\n",
    "\n",
    "Nous allons maintenant généraliser le processus en déployant notre solution <br />\n",
    "sur un réel cluster de machines et nous travaillerons désormais sur la totalité <br />\n",
    "des 22819 images de notre dossier \"Test\"."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "NYNm2MsX5VNBkTyApQmGfr",
     "report_properties": {
      "rowId": "6aR6SZqkA64cxqsK2gb9g8"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 4. Déploiement de la solution sur le cloud\n",
    "\n",
    "Maintenant que nous avons vérifié que notre solution fonctionne, <br />\n",
    "il est temps de la <u>déployer à plus grande échelle sur un vrai cluster de machines</u>.\n",
    "\n",
    "**Attention**, *je travaille sous Linux avec une version Ubuntu, <br />\n",
    "les commandes décrites ci-dessous sont donc réalisées <br />\n",
    "exclusivement dans cet environnement.*\n",
    "\n",
    "<u>Plusieurs contraintes se posent</u> :\n",
    " 1. Quel prestataire de Cloud choisir ?\n",
    " 2. Quelles solutions de ce prestataire adopter ?\n",
    " 3. Où stocker nos données ?\n",
    " 4. Comment configurer nos outils dans ce nouvel environnement ?\n",
    " \n",
    "## 4.1 Choix du prestataire cloud : AWS\n",
    "\n",
    "Le prestataire le plus connu et qui offre à ce jour l'offre <br />\n",
    "la plus large dans le cloud computing est **Amazon Web Services** (AWS).<br />\n",
    "Certaines de leurs offres sont parfaitement adaptées à notre problématique <br />\n",
    "et c'est la raison pour laquelle j'utiliserai leurs services.\n",
    "\n",
    "L'objectif premier est de pouvoir, grâce à AWS, <u>louer de la puissance de calcul à la demande</u>. <br />\n",
    "L'idée étant de pouvoir, quel que soit la charge de travail, <br />\n",
    "obtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, <br />\n",
    "même si le volume de données venait à fortement augmenter.\n",
    "\n",
    "De plus, la capacité d'utiliser cette puissance de calcul à la demande <br />\n",
    "permet de diminuer drastiquement les coûts si l'on compare les coûts d'une location <br />\n",
    "de serveur complet sur une durée fixe (1 mois, 1 année par exemple).\n",
    "\n",
    "## 4.2 Choix de la solution technique : EMR\n",
    "\n",
    "<u>Plusieurs solutions s'offre à nous</u> :\n",
    "1. Solution **IAAS** (Infrastructure AS A Service)\n",
    " - Dans cette configuration **AWS** met à notre disposition des serveurs vierges <br />\n",
    "   sur lequel nous avons un accès en administrateur, ils sont nommés **instance EC2**.<br />\n",
    "   Pour faire simple, nous pouvons avec cette solution reproduire pratiquement <br />\n",
    "   à l'identique la solution mis en œuvre en local sur notre machine.<br />\n",
    "   <u>On installe nous-même l'intégralité des outils puis on exécute notre script</u> :\n",
    "  - Installation de **Spark**, **Java** etc.\n",
    "  - Installation de **Python** (via Anaconda par exemple)\n",
    "  - Installation de **Jupyter Notebook**\n",
    "  - Installation des **librairies complémentaires**\n",
    "  - Il faudra bien évidement veiller à **implémenter les librairies \n",
    "    nécessaires à toutes les machines (workers) du cluster**\n",
    "  - <u>Avantages</u> :\n",
    "      - Liberté totale de mise en œuvre de la solution\n",
    "      - Facilité de mise en œuvre à partir d'un modèle qui s'exécute en local sur une machine Linux\n",
    "  - <u>Inconvénients</u> :\n",
    "      - Cronophage\n",
    "          - Nécessité d'installer et de configurer toute la solution\n",
    "      - Possible problèmes techniques à l'installation des outils (des problématiques qui <br />\n",
    "        n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n",
    "      - Solution non pérenne dans le temps, il faudra veiller à la mise à jour des outils <br />\n",
    "        et éventuellement devoir réinstaller Spark, Java etc. \n",
    "2. Solution **PAAS** (Plateforme As A Service)\n",
    " - **AWS** fournit énormément de services différents, dans l'un de ceux-là <br />\n",
    "   il existe une offre qui permet de louer des **instances EC2** <br />\n",
    "   avec des applications préinstallées et configurées : il s'agit du **service EMR**.\n",
    " - **Spark** y sera déjà installé\n",
    " - Possibilité de demander l'installation de **Tensorflow** ainsi que **JupyterHub**\n",
    " - Possibilité d'indiquer des **packages complémentaires** à installer <br />\n",
    "   à l'initialisation du serveur **sur l'ensemble des machines du cluster**.\n",
    " - <u>Avantages</u> :\n",
    "     - Facilité de mise en œuvre\n",
    "         - Il suffit de très peu de configuration pour obtenir <br />\n",
    "           un environnement parfaitement fonctionnel\n",
    "     - Rapidité de mise en œuvre\n",
    "         - Une fois la première configuration réalisée, il est très facile <br />\n",
    "           et très rapide de recréer des clusters à l'identique qui seront <br />\n",
    "           disponibles presque instantanément (le temps d'instancier les <br />\n",
    "           serveurs soit environ 15/20 minutes)\n",
    "     - Solutions matérielless et logicielles optimisées par les ingénieurs d'AWS\n",
    "         - On sait que les versions installées vont fonctionner <br />\n",
    "           et que l'architecture proposée est optimisée\n",
    "     - Stabilité de la solution\n",
    "    - Solution évolutive\n",
    "        Il est facile d’obtenir à chaque nouvelle instanciation une version à jour <br />\n",
    "        de chaque package, en étant garanti de leur compatibilité avec le reste de l’environnement.\n",
    "  - Plus sécurisé\n",
    "\t- Les éventuels patchs de sécurité seront automatiquement mis à jour <br />\n",
    "      à chaque nouvelle instanciation du cluster EMR.\n",
    " - <u>Inconvénients</u> :\n",
    "     - Peut-être un certain manque de liberté sur la version des packages disponibles ? <br />\n",
    "       Même si je n'ai pas constaté ce problème.\n",
    "   \n",
    "\n",
    "Je retiens la solution **PAAS** en choisissant d'utiliser <br />\n",
    "le service **EMR** d'Amazon Web Services.<br />\n",
    "Je la trouve plus adaptée à notre problématique et permet <br />\n",
    "une mise en œuvre qui soit à la fois plus rapide et <br />\n",
    "plus efficace que la solution IAAS.\n",
    "\n",
    "## 4.3 Choix de la solution de stockage des données : Amazon S3\n",
    "\n",
    "<u>Amazon propose une solution très efficace pour la gestion du stockage des données</u> : **Amazon S3**. <br />\n",
    "S3 pour Amazon Simple Storage Service.\n",
    "\n",
    "Il pourrait être tentant de stocker nos données sur l'espace alloué par le serveur **EC2**, <br />\n",
    "mais si nous ne prenons aucune mesure pour les sauvegarder ensuite sur un autre support, <br />\n",
    "<u>les données seront perdues</u> lorsque le serveur sera résilié (on résilie le serveur lorsqu'on <br />\n",
    "ne s'en sert pas pour des raisons de coût).<br />\n",
    "De fait, si l'on décide d'utiliser l'espace disque du serveur EC2 il faudra imaginer <br />\n",
    "une solution pour sauvegarder les données avant la résiliation du serveur.\n",
    "De plus, nous serions exposés à certaines problématiques si nos données venaient à <br />\n",
    "**saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n",
    "\n",
    "<u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces problématiques</u>. <br />\n",
    "L'espace disque disponible est **illimité**, et il est **indépendant de nos serveurs EC2**. <br />\n",
    "L'accès aux données est **très rapide** car nous restons dans l'environnement d'AWS <br />\n",
    "et nous prenons soin de <u>choisir la même région pour nos serveurs **EC2** et **S3**</u>.\n",
    "\n",
    "De plus, comme nous le verrons <u>il est possible d'accéder aux données sur **S3** <br />\n",
    "    de la même manière que l'on **accède aux données sur un disque local**</u>.<br />\n",
    "Nous utiliserons simplement un **PATH au format s3://...** .\n",
    "\n",
    "## 4.4 Configuration de l'environnement de travail\n",
    "\n",
    "La première étape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/),<br />\n",
    "il s'agit de l'**interface en ligne de commande d'AWS**.<br />\n",
    "Elle nous permet d'**interagir avec les différents services d'AWS**, comme **S3** par exemple.\n",
    "\n",
    "Pour pouvoir utiliser **AWS Cli**, il faut le configurer en créant préalablement <br />\n",
    "un utilisateur à qui on donnera les autorisations dont nous aurons besoin.<br />\n",
    "Dans ce projet il faut que l'utilisateur ait à minima un contrôle total sur le service S3.\n",
    "\n",
    "<u>La gestion des utilisateurs et de leurs droits s'effectue via le service **AMI**</u> d'AWS.\n",
    "\n",
    "Une fois l'utilisateur créé et ses autorisations configurées nous créons une **paire de clés** <br />\n",
    "qui nous permettra de nous **connecter sans à avoir à devoir saisir systématiquement notre login/mot de passe**.<br />\n",
    "\n",
    "Il faut également configurer l'**accès SSH** à nos futurs serveurs EC2. <br />\n",
    "Ici aussi, via un système de clés qui nous dispense de devoir nous authentifier \"à la main\" à chaque connexion.\n",
    "\n",
    "Toutes ses étapes de configuration sont parfaitement décrites <br />\n",
    "dans le cours du projet: [Réalisez des calculs distribués sur des données massives / Découvrez Amazon Web Services](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308686-decouvrez-amazon-web-services#/id/r-4355822)\n",
    "\n",
    "## 4.5 Upload de nos données sur S3\n",
    "\n",
    "Nos outils sont configurés. <br />\n",
    "Il faut maintenant uploader nos données de travail sur Amazon S3.\n",
    "\n",
    "Ici aussi les étapes sont décrites avec précision <br />\n",
    "dans le cours [Réalisez des calculs distribués sur des données massives / Stockez des données sur S3](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308691-stockez-des-donnees-sur-s3)\n",
    "\n",
    "Je décide de n'uploader que les données contenues dans le dossier **Test** du [jeu de données du projet](https://www.kaggle.com/moltean/fruits/download)\n",
    "\n",
    "\n",
    "La première étape consiste à **créer un bucket sur S3** <br />\n",
    "dans lequel nous uploaderons les données du projet:\n",
    "- **aws s3 mb s3://p8-data**\n",
    "\n",
    "On vérifie que le bucket à bien été créé\n",
    "- **aws s3 ls**\n",
    " - Si le nom du bucket s'affiche alors c'est qu'il a été correctement créé.\n",
    "\n",
    "On copie ensuite le contenu du dossier \"**Test**\" <br />\n",
    "dans un répertoire \"**Test**\" sur notre bucket \"**p8-data**\":\n",
    "1. On se place à l'intérieur du répertoire **Test**\n",
    "2. **aws sync . s3://p8-data/Test**\n",
    "\n",
    "La commande **sync** est utile pour synchroniser deux répertoires.\n",
    "\n",
    "<u>Nos données du projet sont maintenant disponibles sur Amazon S3</u>.\n",
    "\n",
    "## 4.6 Configuration du serveur EMR\n",
    "\n",
    "Une fois encore, le cours [Réalisez des calculs distribués sur des données massives / Déployez un cluster de calculs distribués](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> détaille l'essentiel des étapes pour lancer un cluster avec **EMR**.\n",
    "\n",
    "<u>Je détaillerai ici les étapes particulières qui nous permettent <br />\n",
    "de configurer le serveur selon nos besoins</u> :\n",
    "\n",
    "1. Cliquez sur Créer un cluster\n",
    "![Créer un cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_creer.png)\n",
    "2. Cliquez sur Accéder aux options avancées\n",
    "![Créer un cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_options_avancees.png)\n",
    "\n",
    "### 4.6.1 Étape 1 : Logiciels et étapes\n",
    "\n",
    "#### 4.6.1.1 Configuration des logiciels\n",
    "\n",
    "<u>Sélectionnez les packages dont nous aurons besoin comme dans la capture d'écran</u> :\n",
    "1. Nous sélectionnons la dernière version d'**EMR**, soit la version **6.3.0** au moment où je rédige ce document\n",
    "2. Nous cochons bien évidement **Hadoop** et **Spark** qui seront préinstallés dans leur version la plus récente\n",
    "3. Nous aurons également besoin de **TensorFlow** pour importer notre modèle et réaliser le **transfert learning**\n",
    "4. Nous travaillerons enfin avec un **notebook Jupyter** via l'application **JupyterHub**<br />\n",
    " - Comme nous le verrons dans un instant nous allons <u>paramétrer l'application afin que les notebooks</u>, <br />\n",
    "   comme le reste de nos données de travail, <u>soient enregistrés directement sur S3</u>.\n",
    "![Créer un cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_configuration_logiciels.png)\n",
    "\n",
    "#### 4.6.1.2 Modifier les paramètres du logiciel\n",
    "\n",
    "<u>Paramétrez la persistance des notebooks créés et ouvert via JupyterHub</u> :\n",
    "- On peut à cette étape effectuer des demandes de paramétrage particulières sur nos applications. <br />\n",
    "  L'objectif est, comme pour le reste de nos données de travail, <br />\n",
    "  d'éviter toutes les problématiques évoquées précédemment. <br />\n",
    "  C'est l'objectif à cette étape, <u>nous allons enregistrer <br />\n",
    "  et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme <br />\n",
    "  ce serait le cas dans la configuration par défaut de JupyterHub) mais <br />\n",
    "  <u>directement sur **Amazon S3**</u>.\n",
    "- <u>deux solutions sont possibles pour réaliser cela</u> :\n",
    " 1. Créer un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d’accès au fichier JSON\n",
    " 2. Rentrez directement la configuration au format JSON\n",
    " \n",
    "J'ai personnellement créé un fichier JSON lors de la création de ma première instance EMR, <br />\n",
    "puis lorsqu'on décide de cloner notre serveur pour en recréer un facilement à l'identique, <br />\n",
    "la configuration du fichier JSON se retrouve directement copié comme dans la capture ci-dessous.\n",
    "\n",
    "<u>Voici le contenu de mon fichier JSON</u> :  [{\"classification\":\"jupyter-s3-conf\",\"properties\":{\"s3.persistence.bucket\":\"p8-data\",\"s3.persistence.enabled\":\"true\"}}]\n",
    " Appuyez ensuite sur \"**Suivant**\"\n",
    "![Modifier les paramètres du logiciel](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_parametres_logiciel.png)\n",
    "\n",
    "### 4.6.2 Étape 2 : Matériel\n",
    "\n",
    "A cette étape, laissez les choix par défaut. <br />\n",
    "<u>L'important ici est la sélection de nos instances</u> :\n",
    "\n",
    "1. je choisi les instances de type **M5** qui sont des **instances de type équilibrés**\n",
    "2. je choisi le type **xlarge** qui est l'instance la **moins onéreuse disponible**\n",
    " [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
    "3. Je sélectionne **1 instance Maître** (le driver) et **2 instances Principales** (les workeurs) <br />\n",
    "   soit **un total de 3 instance EC2**.\n",
    "![Choix du materiel](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_materiel.png)\n",
    "\n",
    "### 4.6.3 Étape 3 : Paramètres de cluster généraux\n",
    "\n",
    "#### 4.6.3.1 Options générales\n",
    "<u>La première chose à faire est de donner un nom au cluster</u> :<br />\n",
    "*J'ai également décoché \"Protection de la résiliation\" pour des raisons pratiques.*\n",
    "    \n",
    "![Nom du Cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_nom_cluster.png)\n",
    "\n",
    "#### 4.6.3.2 Actions d'amorçage\n",
    "\n",
    "Nous allons à cette étape **choisir les packages manquants à installer** et qui <br />\n",
    "nous serons utiles dans l'exécution de notre notebook.<br />\n",
    "<u>L'avantage de réaliser cette étape maintenant est que les packages <br />\n",
    "installés le seront sur l'ensemble des machines du cluster</u>.\n",
    "\n",
    "La procédure pour créer le fichier **bootstrap** qui contient <br />\n",
    "l'ensemble des instructions permettant d'installer tous <br />\n",
    "les packages dont nous aurons besoin est expliqué dans <br />\n",
    "le cours [Réalisez des calculs distribués sur des données massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n",
    "\n",
    "Nous créons donc un fichier nommé \"**bootstrap-emr.sh**\" que nous <u>uploadons <br />\n",
    "sur S3</u>(je l’installe à la racine de mon **bucket \"p8-data\"**) et nous l'ajoutons <br />\n",
    "comme indiqué dans la capture d'écran ci-dessous:\n",
    "![Actions d'amorcage](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_amorcage.png)\n",
    "\n",
    "Voici le contenu du fichier **bootstrap-emr.sh**<br />\n",
    "Comme on peut le constater il s'agit simplement de commande \"**pip install**\" <br />\n",
    "pour **installer les bibliothèques manquantes** comme réalisé en local.<br />\n",
    "Une fois encore, <u>il est nécessaire de réaliser ces actions à cette étape</u> <br />\n",
    "pour que <u>les packages soient installés sur l'ensemble des machines du cluster</u> <br />\n",
    "et non pas uniquement sur le driver, comme cela serait le cas si nous exécutions <br />\n",
    "ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connecté au driver).\n",
    "![Contenu du fichier bootstrap](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_bootstrap.png)\n",
    "\n",
    "**setuptools** et **pip** sont mis à jour pour éviter une problématique <br />\n",
    "avec l'installation du package **pyarrow**.<br />\n",
    "**Pandas** a eu droit à une mise à jour majeur (1.3.0) il y a moins d'une semaine <br />\n",
    "au moment de la rédaction de ce notebook, et la nouvelle version de **Pandas** <br />\n",
    "nécessite une version plus récente de **Numpy** que la version installée par <br />\n",
    "défaut (1.16.5) à l'initialisation des instances **EC2**. <u>Il ne semble pas <br />\n",
    "possible d'imposer une autre version de Numpy que celle installé par <br />\n",
    "défaut</u> même si on force l'installation d'une version récente de **Numpy** <br />\n",
    "(en tout cas, ni simplement ni intuitivement).<br />\n",
    "La mise à jour étant très récente <u>la version de **Numpy** n'est pas encore <br />\n",
    "mise à jour sur **EC2**</u> mais on peut imaginer que ce sera le cas très rapidement <br />\n",
    "et il ne sera plus nécessaire d'imposer une version spécifique de **Pandas**.<br />\n",
    "En attendant, je demande <u>l'installation de l'avant dernière version de **Pandas (1.2.5)**</u>\n",
    "\n",
    "On clique ensuite sur ***Suivant***\n",
    "\n",
    "### 4.6.4 Étape 4 : Sécurité\n",
    "\n",
    "#### 4.6.4.1 Options de sécurité\n",
    "\n",
    "A cette étape nous sélectionnons la **paire de clés EC2** créé précédemment. <br />\n",
    "Elle nous permettra de se connecter en **ssh** à nos **instances EC2** <br />\n",
    "sans avoir à entrer nos login/mot de passe.<br />\n",
    "On laisse les autres paramètres par défaut. <br />\n",
    "Et enfin, on clique sur \"***Créer un cluster***\"\n",
    " \n",
    "![EMR Sécurité](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_securite.png)\n",
    "\n",
    "## 4.7 Instanciation du serveur\n",
    "\n",
    "Il ne nous reste plus qu'à attendre que le serveur soit prêt. <br />\n",
    "Cette étape peut prendre entre **15 et 20 minutes**.\n",
    "\n",
    "<u>Plusieurs étapes s'enchaîne, on peut suivre l'avancé du statut du **cluster EMR**</u> :\n",
    "\n",
    "![Instanciation étape 1](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_instanciation_01.png)\n",
    "![Instanciation étape 2](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_instanciation_02.png)\n",
    "![Instanciation étape 3](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_instanciation_03.png)\n",
    "\n",
    "<u>Lorsque le statut affiche en vert: \"**En attente**\" cela signifie que l'instanciation <br />\n",
    "s'est bien déroulée et que notre serveur est prêt à être utilisé</u>. \n",
    "\n",
    "## 4.8 Création du tunnel SSH à l'instance EC2 (Maître)\n",
    "\n",
    "### 4.8.1 Création des autorisations sur les connexions entrantes\n",
    "\n",
    "<u>Nous souhaitons maintenant pouvoir accéder à nos applications</u> :\n",
    " - **JupyterHub** pour l'exécution de notre notebook\n",
    " - **Serveur d'historique Spark** pour le suivi de l'exécution <br />\n",
    "   des tâches de notre script lorsqu'il sera lancé\n",
    " \n",
    "Cependant, <u>ces applications ne sont accessibles que depuis le réseau local du driver</u>, <br />\n",
    "et pour y accéder nous devons **créer un tunnel SSH vers le driver**.\n",
    "\n",
    "Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. <br />\n",
    "<u>Pour ouvrir le port 22 qui correspond au port sur lequel écoute le serveur SSH, <br />\n",
    "il faut modifier le **groupe de sécurité EC2 du driver**</u>.\n",
    "\n",
    "Cette étape est décrite dans le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512): \n",
    "\n",
    "*Il faudra que l'on se connecte en SSH au driver de notre cluster. <br />\n",
    "Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. <br />\n",
    "Pour ouvrir le port 22 qui correspond au port sur lequel écoute le serveur SSH, <br />\n",
    "il faut modifier le groupe de sécurité EC2 du driver. Sur la page de la console <br />\n",
    "consacrée à EC2, dans l'onglet \"Réseau et sécurité\", cliquez sur \"Groupes de sécurité\". <br />\n",
    "Vous allez devoir modifier le groupe de sécurité d’ElasticMapReduce-Master. <br />\n",
    "Dans l'onglet \"Entrant\", ajoutez une règle SSH dont la source est \"N'importe où\" <br />\n",
    "(ou \"Mon IP\" si vous disposez d'une adresse IP fixe).*\n",
    "\n",
    "![Configuration autorisation ports entrants pour ssh](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_config_ssh_01.png)\n",
    "\n",
    "<u>Une fois cette étape réalisée vous devriez avoir une configuration semblable à la mienne</u> :\n",
    "\n",
    "![Configuration ssh terminée](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_config_ssh_02.png)\n",
    "\n",
    "### 4.8.2 Création du tunnel ssh vers le Driver\n",
    "\n",
    "On peut maintenant établir le **tunnel SSH** vers le **Driver**. <br />\n",
    "Pour cela on récupère les informations de connexion fournis par Amazon <br />\n",
    "depuis la page du service EMR / Cluster / onglet Récapitulatif en <br />\n",
    "cliquant sur \"**Activer la connexion Web**\"\n",
    "\n",
    "![Activer la connexion Web](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_tunnel_ssh_01.png)\n",
    "\n",
    "<u>On récupère ensuite la commande fournis par Amazon pour **établir le tunnel SSH**</u> :\n",
    "\n",
    "![Récupérer la commande pour établir le tunnel ssh](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_tunnel_ssh_02.png)\n",
    "\n",
    "<u>Dans mon cas, la commande ne fonctionne pas tel</u> quel et j'ai du **l'adapter à ma configuration**. <br />\n",
    "La **clé ssh** se situe dans un dossier \"**.ssh**\" elle-même située dans <br />\n",
    "mon **répertoire personnel** dont le symbole est, sous Linux, identifié par un tilde \"**~**\".\n",
    "\n",
    "Ayant suivi le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives) <br />\n",
    "j'ai choisi d'utiliser le port **5555** au lieu du **8157**, même si le choix n'est pas très important.<br />\n",
    "    j'ai également rencontré un <u>problème de compatibilité</u> avec <br />\n",
    "l'argument \"**-N**\" (liste des arguments et leur significations <br />\n",
    "disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) j'ai décidé de simplement le supprimer.\n",
    "\n",
    "<u>Finalement, j'utilise la commande suivante dans un terminal pour établir <br />\n",
    "    mon tunnel ssh (seul l'URL change d'une instance à une autre)</u> : <br />\n",
    "\"**ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com**\"\n",
    "\n",
    "<u>On inscrit \"**yes**\" pour valider la connexion et si <br />\n",
    "    la connexion est établit on obtient le résultat suivant</u> :\n",
    "\n",
    "![Création du tunnel SSH](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_connexion_ssh_01.png)\n",
    "\n",
    "Nous avons **correctement établi le tunnel ssh avec le driver** sur le port \"5555\".\n",
    "\n",
    "### 4.8.3 Configuration de FoxyProxy\n",
    "\n",
    "Une dernière étape est nécessaire pour accéder à nos applications, <br />\n",
    "en demandant à notre navigateur d'emprunter le tunnel ssh.<br />\n",
    "J'utilise pour cela **FoxyProxy**.\n",
    "[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n",
    "\n",
    "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut à gauche <br />\n",
    "puis renseigner les éléments comme dans la capture ci-dessous :\n",
    "\n",
    "![Configuration FoxyProxy Etape 1](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_foxyproxy_config_01.png)\n",
    "\n",
    "<u>On obtient le résultat ci-dessous</u> :\n",
    "\n",
    "![Configuration FoxyProxy Etape 2](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_foxyproxy_config_02.png)\n",
    "\n",
    "\n",
    "### 4.8.4 Accès aux applications du serveur EMR via le tunnel ssh\n",
    "\n",
    "\n",
    "<u>Avant d'établir notre **tunnel ssh** nous avions ça</u> :\n",
    "\n",
    "![avant tunnel ssh](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_tunnel_ssh_avant.png)\n",
    "\n",
    "<u>On active le **tunnel ssh** comme vu précédemment puis on demande <br />\n",
    "à notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
    "\n",
    "![FoxyProxy activation](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_foxyproxy_activation.png)\n",
    "\n",
    "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
    "\n",
    "![avant tunnel ssh](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_tunnel_ssh_apres.png)\n",
    "\n",
    "## 4.9 Connexion au notebook JupyterHub\n",
    "\n",
    "Pour se connecter à **JupyterHub** en vue d'exécuter notre **notebook**, <br />\n",
    "il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparu <br />\n",
    "depuis que nous avons configuré le **tunnel ssh** et **foxyproxy** sur <br />\n",
    "notre navigateur (actualisez la page si ce n’est pas le cas).\n",
    "\n",
    "![Démarrage de JupyterHub](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_jupyterhub_connexion_01.png)\n",
    "\n",
    "On passe les éventuels avertissements de sécurité puis <br />\n",
    "nous arrivons sur une page de connexion.\n",
    "    \n",
    "<u>On se connecte avec les informations par défaut</u> :\n",
    " - <u>login</u>: **jovyan**\n",
    " - <u>password</u>: **jupyter**\n",
    " \n",
    "![Connexion à JupyterHub](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_jupyterhub_connexion_02.png)\n",
    "\n",
    "Nous arrivons ensuite dans un dossier vierge de notebook.<br />\n",
    "Il suffit d'en créer un en cliquant sur \"**New**\" en haut à droite.\n",
    "\n",
    "![Liste et création des notebook](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_jupyterhub_creer_notebooks.png)\n",
    "\n",
    "Il est également possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
    "\n",
    "Grace à la <u>**persistance** paramétrée à l'instanciation du cluster <br />\n",
    "nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
    "\n",
    "![Notebook stockés sur S3](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_jupyterhub_S3.png)\n",
    "\n",
    "Je décide d'**importer un notebook déjà rédigé en local directement <br />\n",
    "sur S3** et je l'ouvre depuis **l'interface JupyterHub**.\n",
    "\n",
    "## 4.10 Exécution du code\n",
    "\n",
    "Je décide d'exécuter cette partie du code depuis **JupyterHub hébergé sur notre cluster EMR**.<br />\n",
    "Pour ne pas alourdir inutilement les explications du **notebook**, je ne réexpliquerai pas les étapes communes <br />\n",
    "que nous avons déjà vues dans la première partie où l'on a exécuté le code localement sur notre machine virtuelle Ubuntu.\n",
    "\n",
    "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
    "\n",
    "**En utilisant ce kernel, une session spark est créé à l'exécution de la première cellule**. <br />\n",
    "Il n'est donc **plus nécessaire d'exécuter le code \"spark = (SparkSession ...\"** comme lors <br />\n",
    "de l'exécution de notre notebook en local sur notre VM Ubuntu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "z2M4ZavSEpetXoW1dVphyY",
     "report_properties": {
      "rowId": "h4GU6Lckm52rAcmJf4GDPV"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 4.10.1 Démarrage de la session Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "teVkS1FNBKHxwzI6gLnGYd",
     "report_properties": {
      "rowId": "rFqazkyGdybmJpKXOe3oRt"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1682111375869_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-19-249.eu-west-1.compute.internal:20888/proxy/application_1682111375869_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-17-193.eu-west-1.compute.internal:8042/node/containerlogs/container_1682111375869_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# L'exécution de cette cellule démarre l'application Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "fSXHcK2IVwOUp39Hzk8r65",
     "report_properties": {
      "rowId": "ahyS4BipkPz8gvutSRrwCe"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Affichage des informations sur la session en cours et liens vers Spark UI</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "bVQjKmzRMfE97DG8oOBNaa",
     "report_properties": {
      "rowId": "oXyKqet752h3FTH3IJSpXb"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'driverMemory': '1000M', 'executorCores': 2, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>0</td><td>application_1682111375869_0001</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-19-249.eu-west-1.compute.internal:20888/proxy/application_1682111375869_0001/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-17-193.eu-west-1.compute.internal:8042/node/containerlogs/container_1682111375869_0001_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "S95R6BZv2n6yVJOIJbdKXQ",
     "report_properties": {
      "rowId": "bSMuHHfW3q6GiRvo2PCfql"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 4.10.2 Installation des packages\n",
    "\n",
    "Les packages nécessaires ont été installé via l'étape de **bootstrap** à l'instanciation du serveur.\n",
    "\n",
    "### 4.10.3 Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "DAy6C5Fslx6LYt0OsU9kp6",
     "report_properties": {
      "rowId": "d3PHlIehW9V1DjVKACqwpL"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import io\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.applications.mobilenet_v2 import MobileNetV2, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras import Model\n",
    "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "nFaW0NbOumOiX66VpqgPFW",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Spark -- test 01:lib\n",
    "# Spark.\n",
    "#import findspark\n",
    "#import spark\n",
    "#findspark.init()\n",
    "\n",
    "# Pyspark.\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "#Spark -- test 02:lib\n",
    "# ML.\n",
    "from pyspark.ml.image import ImageSchema\n",
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# DL.\n",
    "#from sparkdl import DeepImageFeaturizer\n",
    "\n",
    "# Functions.\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType, ArrayType\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "4LKMTKrmklZIa28Mvlt7eb",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version des librairies utilis?es :\n",
      "tensorflow    : 2.11.0\n",
      "pyspark       : 3.3.1+amzn.0.dev0\n",
      "PIL           : 9.5.0\n",
      "Numpy         : 1.20.0\n",
      "Pandas        : 1.3.5"
     ]
    }
   ],
   "source": [
    "#import now\n",
    "import datetime\n",
    "#import matplotlib\n",
    "#import sys\n",
    "# Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql.functions import element_at, split, col, pandas_udf, PandasUDFType, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Tensorflow Keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "\n",
    "# Gestion des images\n",
    "import PIL\n",
    "from PIL import Image\n",
    "\n",
    "# Taches ML\n",
    "from pyspark.ml.image import ImageSchema\n",
    "\n",
    "# Réduction de dimension - PCA\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT, DenseVector\n",
    "\n",
    "# Modélisation\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Versions\n",
    "print('Version des librairies utilisées :')\n",
    "print('tensorflow    : ' + tf.__version__)\n",
    "print('pyspark       : ' + pyspark.__version__)\n",
    "print('PIL           : ' + PIL.__version__)\n",
    "print('Numpy         : ' + np.__version__)\n",
    "print('Pandas        : ' + pd.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "JepwhUy7T430SBJs8cKHGi",
     "report_properties": {
      "rowId": "EbJU2gdRYQ1TJbDCe1iR62"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 4.10.4 Définition des PATH pour charger les images et enregistrer les résultats\n",
    "\n",
    "Nous accédons directement à nos **données sur S3** comme si elles étaient **stockées localement**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "ZWHfcO9L9ceECLkioo27xe",
     "report_properties": {
      "rowId": "wmHeRkXkeaNn2sNXO5M9tt"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PATH:        s3://sourceimage\n",
      "PATH_Data:   s3://sourceimage/Test\n",
      "PATH_Result: s3://sourceimage/Results2"
     ]
    }
   ],
   "source": [
    "#Mise en place des différents chemins // Paths : \n",
    "PATH = 's3://sourceimage'\n",
    "PATH_Data = PATH+'/Test'\n",
    "PATH_Result = PATH+'/Results2'\n",
    "print('PATH:        '+\\\n",
    "      PATH+'\\nPATH_Data:   '+\\\n",
    "      PATH_Data+'\\nPATH_Result: '+PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Dy10AJqsyDgqpg7rgcLwub",
     "report_properties": {
      "rowId": "SeecwiraXym9aa7KIc1s7U"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 4.10.5 Traitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "CPTN0R04dox4hb5OyFEVNM",
     "report_properties": {
      "rowId": "LYC50c5hhg9OtkEYwKk5Rh"
     },
     "type": "MD"
    }
   },
   "source": [
    "#### 4.10.5.1 Chargement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "nL7i17hbbzsbYcc9I3azco",
     "type": "MD"
    }
   },
   "source": [
    "Details code : A1\n",
    "- La première ligne de code utilise la méthode \"read\" de Spark pour créer un DataFrame à partir des fichiers binaires. La méthode format est utilisée pour indiquer que les fichiers sont des fichiers binaires.\n",
    "  \n",
    "- La deuxième ligne de code utilise la méthode \"option\" pour filtrer les fichiers à charger. La méthode \"pathGlobFilter\" est utilisée pour spécifier que seuls les fichiers avec l'extension \".jpg\" doivent être chargés.\n",
    "  \n",
    "- La troisième ligne de code utilise à nouveau la méthode \"option\", mais cette fois pour spécifier que la recherche de fichiers doit être récursive, c'est-à-dire que tous les sous-répertoires doivent également être explorés.\n",
    "- La dernière ligne de code spécifie \"le chemin\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Opg6aWdlUruG2VPhJLQgKi",
     "report_properties": {
      "rowId": "1nYLGXYbYYm2XnN4KYhAkO"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Voir Détails code A1 :\n",
    "images = spark.read.format(\"binaryFile\") \\\n",
    "  .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
    "  .option(\"recursiveFileLookup\", \"true\") \\\n",
    "  .load(PATH_Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "nrmEVuzbUSV3rrM8aep88S",
     "report_properties": {
      "rowId": "TtazPD6jovG3QYXbhExQNY"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+------+--------------------+\n",
      "|                path|   modificationTime|length|             content|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "|s3://sourceimage/...|2023-04-21 07:55:27|  7353|[FF D8 FF E0 00 1...|\n",
      "|s3://sourceimage/...|2023-04-21 07:55:28|  7350|[FF D8 FF E0 00 1...|\n",
      "|s3://sourceimage/...|2023-04-21 07:55:28|  7349|[FF D8 FF E0 00 1...|\n",
      "|s3://sourceimage/...|2023-04-21 07:55:27|  7348|[FF D8 FF E0 00 1...|\n",
      "|s3://sourceimage/...|2023-04-21 07:55:54|  7328|[FF D8 FF E0 00 1...|\n",
      "+--------------------+-------------------+------+--------------------+\n",
      "only showing top 5 rows"
     ]
    }
   ],
   "source": [
    "images.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "uL47zSrRCp0DexWjVWsKCz",
     "report_properties": {
      "rowId": "lYNCrk1M5GsL8sgIOKP8uG"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>Je ne conserve que le **path** de l'image et j'ajoute <br />\n",
    "    une colonne contenant les **labels** de chaque image</u> :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "YEbOxeuvZk740GIhljPA0g",
     "report_properties": {
      "rowId": "0ZcLALXfofiV5eHslh0cTX"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- path: string (nullable = true)\n",
      " |-- modificationTime: timestamp (nullable = true)\n",
      " |-- length: long (nullable = true)\n",
      " |-- content: binary (nullable = true)\n",
      " |-- label: string (nullable = true)\n",
      "\n",
      "None\n",
      "+----------------------------------------------+----------+\n",
      "|path                                          |label     |\n",
      "+----------------------------------------------+----------+\n",
      "|s3://sourceimage/Test/Watermelon/r_106_100.jpg|Watermelon|\n",
      "|s3://sourceimage/Test/Watermelon/r_109_100.jpg|Watermelon|\n",
      "|s3://sourceimage/Test/Watermelon/r_108_100.jpg|Watermelon|\n",
      "|s3://sourceimage/Test/Watermelon/r_107_100.jpg|Watermelon|\n",
      "|s3://sourceimage/Test/Watermelon/r_95_100.jpg |Watermelon|\n",
      "+----------------------------------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "None"
     ]
    }
   ],
   "source": [
    "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
    "print(images.printSchema())\n",
    "print(images.select('path','label').show(5,False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "j5hIhfLb11wcpruUu386SO",
     "report_properties": {
      "rowId": "Nvsc23j395Ue7WCHq1bmeh"
     },
     "type": "MD"
    }
   },
   "source": [
    "#### 4.10.5.2 Préparation du modèle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "XQ31GBhVWiviwDTvYVBGO7",
     "type": "MD"
    }
   },
   "source": [
    "Détails Code : A2\n",
    "La première ligne de code crée une instance du modèle MobileNetV2.\n",
    "\n",
    "- Le paramètre weights='imagenet' indique que les poids pré-entraînés sur la base de données ImageNet doivent être utilisés.\n",
    "\n",
    "- Le paramètre include_top=True indique que la couche de classification entièrement connectée doit être incluse dans le modèle.\n",
    "\n",
    "- Le paramètre input_shape=(224, 224, 3) spécifie la taille de l'entrée du modèle. Les images sont redimensionnées en 224x224 pixels et ont 3 canaux (rouge, vert, bleu).\n",
    "\n",
    "En résumé, le modèle créé par ce code est un réseau de neurones convolutifs (CNN) pré-entraîné sur ImageNet pour la classification d'images. Le modèle prend en entrée des images de taille 224x224x3 et retourne des prédictions de probabilités pour les classes d'objets présents dans la base de données ImageNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "bNAIIHWO5MelleOMvyZesF",
     "report_properties": {
      "rowId": "LMV8U4s9JvXYEhM2dSF3E7"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
      "\r\n",
      "    8192/14536120 [..............................] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "   49152/14536120 [..............................] - ETA: 28s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "   81920/14536120 [..............................] - ETA: 35s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  180224/14536120 [..............................] - ETA: 20s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  245760/14536120 [..............................] - ETA: 17s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  327680/14536120 [..............................] - ETA: 15s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  458752/14536120 [..............................] - ETA: 12s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  655360/14536120 [>.............................] - ETA: 9s \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "  860160/14536120 [>.............................] - ETA: 8s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 1179648/14536120 [=>............................] - ETA: 6s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 1654784/14536120 [==>...........................] - ETA: 4s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 2285568/14536120 [===>..........................] - ETA: 3s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 3186688/14536120 [=====>........................] - ETA: 2s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 4505600/14536120 [========>.....................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 6275072/14536120 [===========>..................] - ETA: 1s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 8060928/14536120 [===============>..............] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      " 9797632/14536120 [===================>..........] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "11804672/14536120 [=======================>......] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "13664256/14536120 [===========================>..] - ETA: 0s\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\n",
      "14536120/14536120 [==============================] - 1s 0us/step"
     ]
    }
   ],
   "source": [
    "# Détails code : A2\n",
    "model = MobileNetV2(weights='imagenet',\n",
    "                    include_top=True,\n",
    "                    input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "UoFKh2kdCbtSHqkimJvm6X",
     "type": "MD"
    }
   },
   "source": [
    "Détails du code : A3\n",
    "Crée une instance d'un nouveau modèle en utilisant la classe Model de Keras.\n",
    "\n",
    "- Le paramètre inputs=model.input indique que la couche d'entrée du nouveau modèle est la même que celle du modèle pré-entraîné existant.\n",
    "\n",
    "- Le paramètre outputs=model.layers[-2].output indique que la sortie du nouveau modèle est la sortie de l'avant-dernière couche du modèle pré-entraîné existant.\n",
    "\n",
    "En résumé, le nouveau modèle créé par ce code est un sous-ensemble du modèle pré-entraîné existant qui prend en entrée les mêmes images et retourne la sortie de l'avant-dernière couche du modèle. Ce type de modèle est souvent utilisé pour extraire des caractéristiques (ou des embeddings) à partir des images, qui peuvent ensuite être utilisées pour d'autres tâches telles que la classification d'images ou la recherche de similarité d'images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "THo1ikYBHOyLUdyG05CkFK",
     "report_properties": {
      "rowId": "kfjRGehRqXwGaBzOIfBzJf"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Voir détails - code : A3\n",
    "new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "vFCKSkfLo0Clwh5HKJ8jKn",
     "type": "MD"
    }
   },
   "source": [
    "Détails du code : B1\n",
    "\n",
    "La ligne de code sc.broadcast() crée un objet Broadcast à partir des poids du modèle en utilisant l'objet sc qui représente le contexte Spark.\n",
    "\n",
    "La méthode get_weights() récupère les poids du modèle nouvellement créé.\n",
    "L'objet Broadcast est utilisé pour diffuser les poids du modèle à tous les nœuds du cluster Spark de manière efficace et rapide. Cela permet aux différents nœuds de traiter les données simultanément sans avoir besoin de copier les données sur chaque nœud.\n",
    "\n",
    "En résumé, cette ligne de code est utilisée pour diffuser les poids du modèle nouvellement créé à travers le cluster Spark afin de les rendre disponibles pour tous les nœuds du cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "NXz1f5lGOvt5C9KPTpU7D3",
     "report_properties": {
      "rowId": "wDuP4kuvtzt1vqUOPCbrDY"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Voir détails : B1\n",
    "brodcast_weights = sc.broadcast(new_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PLHSiwbRsQvUyoOyWkRBOX",
     "report_properties": {
      "rowId": "mRy1dlsRmZXOOn8rcWkkes"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 224, 224, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Conv1 (Conv2D)                 (None, 112, 112, 32  864         ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " bn_Conv1 (BatchNormalization)  (None, 112, 112, 32  128         ['Conv1[0][0]']                  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " Conv1_relu (ReLU)              (None, 112, 112, 32  0           ['bn_Conv1[0][0]']               \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise (Depth  (None, 112, 112, 32  288        ['Conv1_relu[0][0]']             \n",
      " wiseConv2D)                    )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_BN (Ba  (None, 112, 112, 32  128        ['expanded_conv_depthwise[0][0]']\n",
      " tchNormalization)              )                                                                 \n",
      "                                                                                                  \n",
      " expanded_conv_depthwise_relu (  (None, 112, 112, 32  0          ['expanded_conv_depthwise_BN[0][0\n",
      " ReLU)                          )                                ]']                              \n",
      "                                                                                                  \n",
      " expanded_conv_project (Conv2D)  (None, 112, 112, 16  512        ['expanded_conv_depthwise_relu[0]\n",
      "                                )                                [0]']                            \n",
      "                                                                                                  \n",
      " expanded_conv_project_BN (Batc  (None, 112, 112, 16  64         ['expanded_conv_project[0][0]']  \n",
      " hNormalization)                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand (Conv2D)        (None, 112, 112, 96  1536        ['expanded_conv_project_BN[0][0]'\n",
      "                                )                                ]                                \n",
      "                                                                                                  \n",
      " block_1_expand_BN (BatchNormal  (None, 112, 112, 96  384        ['block_1_expand[0][0]']         \n",
      " ization)                       )                                                                 \n",
      "                                                                                                  \n",
      " block_1_expand_relu (ReLU)     (None, 112, 112, 96  0           ['block_1_expand_BN[0][0]']      \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_pad (ZeroPadding2D)    (None, 113, 113, 96  0           ['block_1_expand_relu[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " block_1_depthwise (DepthwiseCo  (None, 56, 56, 96)  864         ['block_1_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_1_depthwise_BN (BatchNor  (None, 56, 56, 96)  384         ['block_1_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_1_depthwise_relu (ReLU)  (None, 56, 56, 96)   0           ['block_1_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_1_project (Conv2D)       (None, 56, 56, 24)   2304        ['block_1_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_1_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_1_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_1_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_2_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_2_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_2_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_2_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_2_depthwise (DepthwiseCo  (None, 56, 56, 144)  1296       ['block_2_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_2_depthwise_BN (BatchNor  (None, 56, 56, 144)  576        ['block_2_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_2_depthwise_relu (ReLU)  (None, 56, 56, 144)  0           ['block_2_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_2_project (Conv2D)       (None, 56, 56, 24)   3456        ['block_2_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_2_project_BN (BatchNorma  (None, 56, 56, 24)  96          ['block_2_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_2_add (Add)              (None, 56, 56, 24)   0           ['block_1_project_BN[0][0]',     \n",
      "                                                                  'block_2_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_3_expand (Conv2D)        (None, 56, 56, 144)  3456        ['block_2_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_3_expand_BN (BatchNormal  (None, 56, 56, 144)  576        ['block_3_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_3_expand_relu (ReLU)     (None, 56, 56, 144)  0           ['block_3_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_3_pad (ZeroPadding2D)    (None, 57, 57, 144)  0           ['block_3_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_3_depthwise (DepthwiseCo  (None, 28, 28, 144)  1296       ['block_3_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_3_depthwise_BN (BatchNor  (None, 28, 28, 144)  576        ['block_3_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_3_depthwise_relu (ReLU)  (None, 28, 28, 144)  0           ['block_3_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_3_project (Conv2D)       (None, 28, 28, 32)   4608        ['block_3_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_3_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_3_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_3_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_4_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_4_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_4_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_4_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_4_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_4_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_4_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_4_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_4_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_4_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_4_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_4_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_4_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_4_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_4_add (Add)              (None, 28, 28, 32)   0           ['block_3_project_BN[0][0]',     \n",
      "                                                                  'block_4_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_5_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_4_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_5_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_5_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_5_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_5_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_5_depthwise (DepthwiseCo  (None, 28, 28, 192)  1728       ['block_5_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_5_depthwise_BN (BatchNor  (None, 28, 28, 192)  768        ['block_5_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_5_depthwise_relu (ReLU)  (None, 28, 28, 192)  0           ['block_5_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_5_project (Conv2D)       (None, 28, 28, 32)   6144        ['block_5_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_5_project_BN (BatchNorma  (None, 28, 28, 32)  128         ['block_5_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_5_add (Add)              (None, 28, 28, 32)   0           ['block_4_add[0][0]',            \n",
      "                                                                  'block_5_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_6_expand (Conv2D)        (None, 28, 28, 192)  6144        ['block_5_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_6_expand_BN (BatchNormal  (None, 28, 28, 192)  768        ['block_6_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_6_expand_relu (ReLU)     (None, 28, 28, 192)  0           ['block_6_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_6_pad (ZeroPadding2D)    (None, 29, 29, 192)  0           ['block_6_expand_relu[0][0]']    \n",
      "                                                                                                  \n",
      " block_6_depthwise (DepthwiseCo  (None, 14, 14, 192)  1728       ['block_6_pad[0][0]']            \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_6_depthwise_BN (BatchNor  (None, 14, 14, 192)  768        ['block_6_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_6_depthwise_relu (ReLU)  (None, 14, 14, 192)  0           ['block_6_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_6_project (Conv2D)       (None, 14, 14, 64)   12288       ['block_6_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_6_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_6_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_6_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_7_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_7_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_7_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_7_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_7_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_7_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_7_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_7_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_7_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_7_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_7_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_7_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_7_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_7_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_7_add (Add)              (None, 14, 14, 64)   0           ['block_6_project_BN[0][0]',     \n",
      "                                                                  'block_7_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_8_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_7_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_8_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_8_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_8_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_8_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_8_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_8_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_8_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_8_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_8_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_8_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_8_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_8_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_8_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_8_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_8_add (Add)              (None, 14, 14, 64)   0           ['block_7_add[0][0]',            \n",
      "                                                                  'block_8_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_9_expand (Conv2D)        (None, 14, 14, 384)  24576       ['block_8_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_9_expand_BN (BatchNormal  (None, 14, 14, 384)  1536       ['block_9_expand[0][0]']         \n",
      " ization)                                                                                         \n",
      "                                                                                                  \n",
      " block_9_expand_relu (ReLU)     (None, 14, 14, 384)  0           ['block_9_expand_BN[0][0]']      \n",
      "                                                                                                  \n",
      " block_9_depthwise (DepthwiseCo  (None, 14, 14, 384)  3456       ['block_9_expand_relu[0][0]']    \n",
      " nv2D)                                                                                            \n",
      "                                                                                                  \n",
      " block_9_depthwise_BN (BatchNor  (None, 14, 14, 384)  1536       ['block_9_depthwise[0][0]']      \n",
      " malization)                                                                                      \n",
      "                                                                                                  \n",
      " block_9_depthwise_relu (ReLU)  (None, 14, 14, 384)  0           ['block_9_depthwise_BN[0][0]']   \n",
      "                                                                                                  \n",
      " block_9_project (Conv2D)       (None, 14, 14, 64)   24576       ['block_9_depthwise_relu[0][0]'] \n",
      "                                                                                                  \n",
      " block_9_project_BN (BatchNorma  (None, 14, 14, 64)  256         ['block_9_project[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_9_add (Add)              (None, 14, 14, 64)   0           ['block_8_add[0][0]',            \n",
      "                                                                  'block_9_project_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_expand (Conv2D)       (None, 14, 14, 384)  24576       ['block_9_add[0][0]']            \n",
      "                                                                                                  \n",
      " block_10_expand_BN (BatchNorma  (None, 14, 14, 384)  1536       ['block_10_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_10_expand_relu (ReLU)    (None, 14, 14, 384)  0           ['block_10_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_10_depthwise (DepthwiseC  (None, 14, 14, 384)  3456       ['block_10_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_10_depthwise_BN (BatchNo  (None, 14, 14, 384)  1536       ['block_10_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_10_depthwise_relu (ReLU)  (None, 14, 14, 384)  0          ['block_10_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_10_project (Conv2D)      (None, 14, 14, 96)   36864       ['block_10_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_10_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_10_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_10_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_11_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_11_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_11_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_11_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_11_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_11_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_11_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_11_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_11_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_11_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_11_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_11_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_11_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_11_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_11_add (Add)             (None, 14, 14, 96)   0           ['block_10_project_BN[0][0]',    \n",
      "                                                                  'block_11_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_12_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_11_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_12_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_12_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_12_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_12_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_12_depthwise (DepthwiseC  (None, 14, 14, 576)  5184       ['block_12_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_12_depthwise_BN (BatchNo  (None, 14, 14, 576)  2304       ['block_12_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_12_depthwise_relu (ReLU)  (None, 14, 14, 576)  0          ['block_12_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_12_project (Conv2D)      (None, 14, 14, 96)   55296       ['block_12_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_12_project_BN (BatchNorm  (None, 14, 14, 96)  384         ['block_12_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_12_add (Add)             (None, 14, 14, 96)   0           ['block_11_add[0][0]',           \n",
      "                                                                  'block_12_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_13_expand (Conv2D)       (None, 14, 14, 576)  55296       ['block_12_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_13_expand_BN (BatchNorma  (None, 14, 14, 576)  2304       ['block_13_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_13_expand_relu (ReLU)    (None, 14, 14, 576)  0           ['block_13_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_13_pad (ZeroPadding2D)   (None, 15, 15, 576)  0           ['block_13_expand_relu[0][0]']   \n",
      "                                                                                                  \n",
      " block_13_depthwise (DepthwiseC  (None, 7, 7, 576)   5184        ['block_13_pad[0][0]']           \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_13_depthwise_BN (BatchNo  (None, 7, 7, 576)   2304        ['block_13_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_13_depthwise_relu (ReLU)  (None, 7, 7, 576)   0           ['block_13_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_13_project (Conv2D)      (None, 7, 7, 160)    92160       ['block_13_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_13_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_13_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_13_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_14_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_14_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_14_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_14_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_14_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_14_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_14_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_14_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_14_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_14_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_14_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_14_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_14_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_14_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_14_add (Add)             (None, 7, 7, 160)    0           ['block_13_project_BN[0][0]',    \n",
      "                                                                  'block_14_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_15_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_14_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_15_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_15_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_15_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_15_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_15_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_15_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_15_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_15_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_15_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_15_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_15_project (Conv2D)      (None, 7, 7, 160)    153600      ['block_15_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_15_project_BN (BatchNorm  (None, 7, 7, 160)   640         ['block_15_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " block_15_add (Add)             (None, 7, 7, 160)    0           ['block_14_add[0][0]',           \n",
      "                                                                  'block_15_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " block_16_expand (Conv2D)       (None, 7, 7, 960)    153600      ['block_15_add[0][0]']           \n",
      "                                                                                                  \n",
      " block_16_expand_BN (BatchNorma  (None, 7, 7, 960)   3840        ['block_16_expand[0][0]']        \n",
      " lization)                                                                                        \n",
      "                                                                                                  \n",
      " block_16_expand_relu (ReLU)    (None, 7, 7, 960)    0           ['block_16_expand_BN[0][0]']     \n",
      "                                                                                                  \n",
      " block_16_depthwise (DepthwiseC  (None, 7, 7, 960)   8640        ['block_16_expand_relu[0][0]']   \n",
      " onv2D)                                                                                           \n",
      "                                                                                                  \n",
      " block_16_depthwise_BN (BatchNo  (None, 7, 7, 960)   3840        ['block_16_depthwise[0][0]']     \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " block_16_depthwise_relu (ReLU)  (None, 7, 7, 960)   0           ['block_16_depthwise_BN[0][0]']  \n",
      "                                                                                                  \n",
      " block_16_project (Conv2D)      (None, 7, 7, 320)    307200      ['block_16_depthwise_relu[0][0]']\n",
      "                                                                                                  \n",
      " block_16_project_BN (BatchNorm  (None, 7, 7, 320)   1280        ['block_16_project[0][0]']       \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " Conv_1 (Conv2D)                (None, 7, 7, 1280)   409600      ['block_16_project_BN[0][0]']    \n",
      "                                                                                                  \n",
      " Conv_1_bn (BatchNormalization)  (None, 7, 7, 1280)  5120        ['Conv_1[0][0]']                 \n",
      "                                                                                                  \n",
      " out_relu (ReLU)                (None, 7, 7, 1280)   0           ['Conv_1_bn[0][0]']              \n",
      "                                                                                                  \n",
      " global_average_pooling2d (Glob  (None, 1280)        0           ['out_relu[0][0]']               \n",
      " alAveragePooling2D)                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,257,984\n",
      "Trainable params: 2,223,872\n",
      "Non-trainable params: 34,112\n",
      "__________________________________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "IrriOozc7arPmg2BnxSeKb",
     "type": "MD"
    }
   },
   "source": [
    "Explication - \"model_fn()\" :\n",
    " \n",
    "Cette boucle crée et renvoie un nouveau modèle à partir du modèle pré-entraîné MobileNetV2 avec les poids distribués à travers le cluster Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "9ekaJjno7j0ifdzCxx0R3g",
     "report_properties": {
      "rowId": "Ub3oMn1enIAAbUS1sVxFJa"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def model_fn():\n",
    "    \"\"\"\n",
    "    Returns a MobileNetV2 model with top layer removed \n",
    "    and broadcasted pretrained weights.\n",
    "    \"\"\"\n",
    "    # Chargement du modèle pré-entraîné MobileNetV2 avec les poids pré-entraînés sur ImageNet\n",
    "    model = MobileNetV2(weights='imagenet',\n",
    "                        include_top=True,\n",
    "                        input_shape=(224, 224, 3))\n",
    "    # Désactivation de la formation de nouveaux poids pour le modèle\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    # Création d'un nouveau modèle avec les mêmes entrées et sorties que le modèle précédent, mais sans la dernière couche    \n",
    "    new_model = Model(inputs=model.input,\n",
    "                  outputs=model.layers[-2].output)\n",
    "    # Attribution des poids pré-entraînés au nouveau modèle\n",
    "    new_model.set_weights(brodcast_weights.value)\n",
    "    # Retourne le nouveau modèle avec les poids pré-entraînés et la dernière couche de classification supprimée\n",
    "    # Ce modèle peut être utilisé pour extraire des caractéristiques à partir des images et peut être utilisé comme point de départ pour entraîner des modèles de classification d'images personnalisés.\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "RjGZ92sjG5SWyIrbi0wue1",
     "report_properties": {
      "rowId": "ktEHLkkUt5ZiKPLzGnjlZs"
     },
     "type": "MD"
    }
   },
   "source": [
    "#### 4.10.5.3 Définition du processus de chargement des images <br/> et application de leur featurisation à travers l'utilisation de pandas UDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "eanf4QUMpvIKuibltXJzYl",
     "type": "MD"
    }
   },
   "source": [
    "Explication : \"preprocess\",\"featurize_series\",\"featurize_udf\" :\n",
    "\n",
    "Ces trois fonctions sont utilisées pour extraire des caractéristiques à partir d'images. Voici un résumé de leurs fonctionnalités :\n",
    "\n",
    "- La fonction preprocess prend une image brute sous forme de bytes et la pré-traite pour la prédiction. Elle utilise la bibliothèque Pillow pour redimensionner l'image en 224x224 pixels, puis convertit l'image en un tableau de pixels et enfin normalise les valeurs des pixels pour qu'elles soient centrées autour de zéro.\n",
    "\n",
    "- La fonction featurize_series prend un modèle de réseau de neurones et une série d'images brutes, et renvoie une série de caractéristiques d'image calculées à partir des couches cachées du modèle. Elle pré-traite les images à l'aide de la fonction preprocess, empile les images pré-traitées dans un tableau numpy, utilise le modèle pour prédire les caractéristiques pour chaque image, puis aplatisse les caractéristiques en un vecteur pour faciliter le stockage dans un DataFrame Spark.\n",
    "\n",
    "- La fonction featurize_udf est une fonction UDF (User-Defined Function) de pandas qui encapsule la fonction featurize_series. Elle est conçue pour être utilisée avec des DataFrames Spark et prend un itérateur de séries d'images brutes. Elle charge le modèle en utilisant la fonction model_fn et itère sur les séries d'images brutes en utilisant featurize_series pour extraire les caractéristiques des images. Les résultats sont renvoyés sous forme de colonne de DataFrame Spark de type ArrayType(FloatType).\n",
    "\n",
    "En utilisant ces fonctions, nous pouvons extraire des caractéristiques à partir de grandes quantités d'images pour les utiliser dans la formation de modèles de classification ou de clustering d'images.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "UaUKULEb64Y3e6gjdUjDQz",
     "report_properties": {
      "rowId": "m9AZ3KuaWchEH3SzdrukEF"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt1/yarn/usercache/livy/appcache/application_1682111375869_0001/container_1682111375869_0001_01_000001/pyspark.zip/pyspark/sql/pandas/functions.py:398: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details."
     ]
    }
   ],
   "source": [
    "def preprocess(content):\n",
    "    \"\"\"\n",
    "    Preprocesses raw image bytes for prediction.\n",
    "    \"\"\"\n",
    "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
    "    arr = img_to_array(img)\n",
    "    return preprocess_input(arr)\n",
    "\n",
    "def featurize_series(model, content_series):\n",
    "    \"\"\"\n",
    "    Featurize a pd.Series of raw images using the input model.\n",
    "    :return: a pd.Series of image features\n",
    "    \"\"\"\n",
    "    input = np.stack(content_series.map(preprocess))\n",
    "    preds = model.predict(input)\n",
    "    # For some layers, output features will be multi-dimensional tensors.\n",
    "    # We flatten the feature tensors to vectors for easier storage in Spark DataFrames.\n",
    "    output = [p.flatten() for p in preds]\n",
    "    return pd.Series(output)\n",
    "\n",
    "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
    "def featurize_udf(content_series_iter):\n",
    "    '''\n",
    "    This method is a Scalar Iterator pandas UDF wrapping our featurization function.\n",
    "    The decorator specifies that this returns a Spark DataFrame column of type ArrayType(FloatType).\n",
    "\n",
    "    :param content_series_iter: This argument is an iterator over batches of data, where each batch\n",
    "                              is a pandas Series of image data.\n",
    "    '''\n",
    "    # With Scalar Iterator pandas UDFs, we can load the model once and then re-use it\n",
    "    # for multiple data batches.  This amortizes the overhead of loading big models.\n",
    "    model = model_fn()\n",
    "    for content_series in content_series_iter:\n",
    "        yield featurize_series(model, content_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "1gqQTCJSr538xMuN2wr8hk",
     "report_properties": {
      "rowId": "hQKazsF03wPZCI6WgORh96"
     },
     "type": "MD"
    }
   },
   "source": [
    "#### 4.10.5.4 Exécutions des actions d'extractions de features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "D9ptenv2zCJ1kTkBiR0G4d",
     "report_properties": {
      "rowId": "47wqTpXNARDyOnSuVkqYO4"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "datalore": {
        "widget_id": "cmNItONbaNCBN7aq8eyuvf"
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": true,
     "hide_output_from_viewers": true,
     "node_id": "mW06TmVfFYt3oLATz2DzgB",
     "type": "MD"
    }
   },
   "source": [
    "Détails du code : B2\n",
    "\n",
    "On utilise la bibliothèque Spark pour extraire des caractéristiques à partir d'images. Voici une explication de ce que chaque ligne de code fait :\n",
    "\n",
    "- \"images\" est une variable qui contient un DataFrame Spark contenant des images. Ce DataFrame a été créé en utilisant la méthode \"spark.read.format\" pour lire des fichiers binaires et la méthode \"load\" pour spécifier le répertoire où les images sont stockées. La méthode option a été utilisée pour spécifier des options de filtrage de fichiers pour limiter les images aux fichiers de type \".jpg\", et pour spécifier que la recherche de fichiers doit être récursive.\n",
    "\n",
    "- \"repartition(24)\" est une méthode qui répartit les données du DataFrame en 24 partitions pour une meilleure efficacité de calcul.\n",
    "\n",
    "- \"select\" est une méthode qui sélectionne certaines colonnes du DataFrame. Dans ce cas, les colonnes \"path\" et \"label\" sont sélectionnées. \"path\" est le chemin d'accès de chaque image et \"label\" est une étiquette associée à chaque image.\n",
    "\n",
    "- \"featurize_udf(\"content\")\" est une méthode qui applique une fonction UDF (User-Defined Function) appelée featurize_udf sur la colonne \"content\" du DataFrame. La colonne \"content\" contient les données brutes de l'image sous forme binaire.\n",
    "\n",
    "- \"alias(\"features\")\" est une méthode qui renomme la colonne de caractéristiques nouvellement créée. Dans ce cas, la colonne est renommée \"features\".\n",
    "\n",
    "Enfin, \"features_df\" est une variable qui contient le nouveau DataFrame Spark résultant de cette opération. Le DataFrame contient les colonnes \"path\", \"label\" et \"features\", où \"features\" contient les caractéristiques extraites de chaque image.\n",
    "\n",
    "En résumé, ce code extrait des caractéristiques à partir d'images en utilisant un modèle de réseau de neurones et stocke les résultats dans un DataFrame Spark pour une analyse ultérieure.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "gLaGhw4dfImYP7mRcWdnp9",
     "report_properties": {
      "rowId": "2ITpGIuI8GkLf8IeigxNCF"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Détails - code : B2\n",
    "features_df = images.repartition(24).select(col(\"path\"),\n",
    "                                            col(\"label\"),\n",
    "                                            featurize_udf(\"content\").alias(\"features\")\n",
    "                                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Zbn3F5ZKCzPMNETBHBSsx1",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4685"
     ]
    }
   ],
   "source": [
    "#Test01\n",
    "features_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "BCu6aIE1UIXFgqQNLyzvec",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+\n",
      "|                path|           label|            features|\n",
      "+--------------------+----------------+--------------------+\n",
      "|s3://sourceimage/...|      Watermelon|[0.60736495, 0.06...|\n",
      "|s3://sourceimage/...|      Watermelon|[1.31946, 0.27604...|\n",
      "|s3://sourceimage/...|      Watermelon|[0.69644827, 0.00...|\n",
      "|s3://sourceimage/...|      Watermelon|[0.11927576, 0.00...|\n",
      "|s3://sourceimage/...|       Raspberry|[0.113048755, 0.8...|\n",
      "|s3://sourceimage/...|       Raspberry|[1.1750436, 1.591...|\n",
      "|s3://sourceimage/...|       Raspberry|[0.6071593, 1.759...|\n",
      "|s3://sourceimage/...|       Raspberry|[0.4382261, 1.787...|\n",
      "|s3://sourceimage/...|     Pomegranate|[2.0188222, 0.0, ...|\n",
      "|s3://sourceimage/...|     Pomegranate|[2.325005, 0.0354...|\n",
      "|s3://sourceimage/...|     Pomegranate|[1.1951542, 0.232...|\n",
      "|s3://sourceimage/...|Strawberry Wedge|[1.1922286, 0.078...|\n",
      "|s3://sourceimage/...|        Rambutan|[0.0, 2.897274, 0...|\n",
      "|s3://sourceimage/...|        Rambutan|[0.0, 3.6554348, ...|\n",
      "|s3://sourceimage/...|          Walnut|[0.023891693, 0.0...|\n",
      "|s3://sourceimage/...|    Potato White|[0.018497009, 0.0...|\n",
      "|s3://sourceimage/...|    Tomato Heart|[0.0, 0.018173646...|\n",
      "|s3://sourceimage/...|      Strawberry|[2.197247, 0.1246...|\n",
      "|s3://sourceimage/...|    Potato Sweet|[0.023939298, 0.0...|\n",
      "|s3://sourceimage/...|    Tomato Heart|[0.05886257, 0.0,...|\n",
      "+--------------------+----------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "#Test02\n",
    "features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "A61NPvW6RKNipVPJUTFR3E",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test03\n",
    "def preprocess_pca(dataframe):\n",
    "  '''\n",
    "     Préparation des données :\n",
    "     - transformation en vecteur dense\n",
    "     - standardisation\n",
    "     param : dataframe : dataframe d'images\n",
    "     return : dataframe avec features vecteur dense standardisé\n",
    "  '''\n",
    "  \n",
    "  # Préparation des données - conversion des données images en vecteur dense\n",
    "  transform_vecteur_dense = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
    "  dataframe = dataframe.withColumn('features_vectors', transform_vecteur_dense('features'))\n",
    "  \n",
    "  # Standardisation obligatoire pour PCA\n",
    "  scaler_std = StandardScaler(inputCol=\"features_vectors\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
    "  model_std = scaler_std.fit(dataframe)\n",
    "  # Mise à l'échelle\n",
    "  dataframe = model_std.transform(dataframe)\n",
    "  \n",
    "  return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "UBJTHRQRHgXschQbis5U6D",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Recherche du nombre de composante expliquant 95% de la variance\n",
    "def recherche_nb_composante(dataframe, nb_comp=400):\n",
    "    '''\n",
    "       Recherche d nombre de composante expliquant 95% de la variance\n",
    "       param : dataframe : dataframe d'images\n",
    "       return : k nombre de composante expliquant 95% de la variance totale\n",
    "    '''\n",
    "    \n",
    "    pca = PCA(k = nb_comp,\n",
    "              inputCol=\"features_scaled\", \n",
    "              outputCol=\"features_pca\")\n",
    " \n",
    "    model_pca = pca.fit(dataframe)\n",
    "    variance = model_pca.explainedVariance\n",
    " \n",
    "    # visuel\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.plot(np.arange(len(variance)) + 1, variance.cumsum(), c=\"red\", marker='o')\n",
    "    plt.xlabel(\"Nb composantes\")\n",
    "    plt.ylabel(\"% variance\")\n",
    "    plt.show(block=False)\n",
    "    \n",
    " \n",
    "    def nb_comp ():\n",
    "      for i in range(500):\n",
    "          a = variance.cumsum()[i]\n",
    "          if a >= 0.95:\n",
    "              print(\"{} composantes principales expliquent au moins 95% de la variance totale\".format(i))\n",
    "              break\n",
    "      return i\n",
    " \n",
    "    k=nb_comp()\n",
    "  \n",
    "    return k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "BNJuJfQczLVZEK7wJEGcj2",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Pré-processing (vecteur dense, standardisation)\n",
    "df_pca = preprocess_pca(features_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Wht02Jrv8p5YK9MbYDbsyY",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "347 composantes principales expliquent au moins 95% de la variance totale"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplot plt\n",
    "# Nombre de composante expliquant 95% de la variance\n",
    "n_components = recherche_nb_composante(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "WQmuWc2aEd7tJcsw8BuD7U",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Réduction de dimension PCA\n",
    "# Entrainement de l'algorithme\n",
    "pca = PCA(k=n_components, inputCol='features_scaled', outputCol='vectors_pca')\n",
    "model_pca = pca.fit(df_pca)\n",
    "\n",
    "# Transformation des images sur les k premières composantes\n",
    "df_reduit = model_pca.transform(df_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "AxAWK9f1kvtvkg5PM8x5rf",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|                path|             label|            features|    features_vectors|     features_scaled|         vectors_pca|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|s3://sourceimage/...|        Watermelon|[0.14366496, 0.14...|[0.14366495609283...|[-0.4290835000613...|[-23.206803575936...|\n",
      "|s3://sourceimage/...|        Watermelon|[0.0, 0.2200655, ...|[0.0,0.2200655043...|[-0.6654355229119...|[-12.989237892500...|\n",
      "|s3://sourceimage/...|        Watermelon|[0.0, 0.13857804,...|[0.0,0.1385780423...|[-0.6654355229119...|[-15.883198571522...|\n",
      "|s3://sourceimage/...|         Raspberry|[0.113048755, 0.8...|[0.11304875463247...|[-0.4794520892006...|[-3.8088351370077...|\n",
      "|s3://sourceimage/...|          Rambutan|[0.4864472, 3.652...|[0.48644718527793...|[0.13484855037405...|[-19.162689787485...|\n",
      "|s3://sourceimage/...|         Raspberry|[0.30788252, 0.28...|[0.30788251757621...|[-0.1589191266485...|[-4.9439331688168...|\n",
      "|s3://sourceimage/...|         Raspberry|[0.6071593, 1.759...|[0.60715931653976...|[0.33343947457900...|[-4.4108504839290...|\n",
      "|s3://sourceimage/...|         Raspberry|[0.4382261, 1.787...|[0.43822610378265...|[0.05551709428750...|[-4.9802326820648...|\n",
      "|s3://sourceimage/...|         Raspberry|[0.9289935, 1.165...|[0.92899352312088...|[0.86290864936379...|[-4.5894209860411...|\n",
      "|s3://sourceimage/...|       Pomegranate|[2.325005, 0.0354...|[2.32500505447387...|[3.15957276517531...|[-1.0302973847720...|\n",
      "|s3://sourceimage/...|       Pomegranate|[1.1951542, 0.232...|[1.19515419006347...|[1.30078587249466...|[-0.3644523481912...|\n",
      "|s3://sourceimage/...|       Pomegranate|[2.1560137, 0.009...|[2.15601372718811...|[2.88155477711154...|[1.29948616097650...|\n",
      "|s3://sourceimage/...|       Pomegranate|[1.3852677, 0.064...|[1.38526773452758...|[1.61355331437767...|[2.72478313388653...|\n",
      "|s3://sourceimage/...|            Walnut|[0.0, 0.007547301...|[0.0,0.0075473007...|[-0.6654355229119...|[-15.287694870909...|\n",
      "|s3://sourceimage/...|          Rambutan|[0.0, 2.897274, 0...|[0.0,2.8972740173...|[-0.6654355229119...|[-17.513885595982...|\n",
      "|s3://sourceimage/...|      Potato White|[0.018497009, 0.0...|[0.01849700883030...|[-0.6350049602150...|[-6.1319501548665...|\n",
      "|s3://sourceimage/...|Tomato not Ripened|[0.0, 0.19918932,...|[0.0,0.1991893202...|[-0.6654355229119...|[-9.3932921695008...|\n",
      "|s3://sourceimage/...|          Rambutan|[0.0, 2.8382127, ...|[0.0,2.8382127285...|[-0.6654355229119...|[-16.851570616755...|\n",
      "|s3://sourceimage/...|        Strawberry|[2.197247, 0.1246...|[2.19724702835083...|[2.94939020757184...|[-13.399867003825...|\n",
      "|s3://sourceimage/...|      Tomato Heart|[0.0, 0.093072094...|[0.0,0.0930720940...|[-0.6654355229119...|[3.17091861360235...|\n",
      "+--------------------+------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "# Visualisation du dataframe réduit\n",
    "df_reduit.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PD0FxzhOzH1ZZ8v8Q3cSBZ",
     "report_properties": {
      "rowId": "nHtMWs5iHte7O9SEpM8YV1"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sourceimage/Results2"
     ]
    }
   ],
   "source": [
    "print(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PMnMljReNqbCKnze6ZYeaf",
     "report_properties": {
      "rowId": "WiMLDG8vZCMKaNoCXXntiv"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sauvegarde des données - AFTER PCA\n",
    "df_reduit.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "KzzThR2KgCAdUd2CTXSPEC",
     "report_properties": {
      "rowId": "iNeQqfVtTz4UKSsLLjZVE6"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      }
     },
     "metadata": {
      "application/vnd.jupyter.widget-view+json": {
       "datalore": {
        "widget_id": "ezeK5TVURIkebddRVMJL3d"
       }
      }
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sauvegarde des données - SANS PCA \n",
    "#features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "5EAljJ96KlBB7YvT1A0LIq",
     "report_properties": {
      "rowId": "DvAM3qJnNlerX4U9A8FqWI"
     },
     "type": "MD"
    }
   },
   "source": [
    "### 4.10.6 Chargement des données enregistrées et validation du résultat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "jUHSjq66MWLVtlCCR3u78i",
     "report_properties": {
      "rowId": "JRf9dt5CGN1ptVcdJOXwDE"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "Tqx4ImUWg9qhhHtWNIuJDW",
     "report_properties": {
      "rowId": "3pcqirjM1PoVjFkwOfYB1m"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            path  ...                                        vectors_pca\n",
      "0  s3://sourceimage/Test/Watermelon/r_80_100.jpg  ...  {'type': 1, 'size': None, 'indices': None, 'va...\n",
      "1  s3://sourceimage/Test/Watermelon/r_66_100.jpg  ...  {'type': 1, 'size': None, 'indices': None, 'va...\n",
      "2  s3://sourceimage/Test/Watermelon/r_85_100.jpg  ...  {'type': 1, 'size': None, 'indices': None, 'va...\n",
      "3   s3://sourceimage/Test/Watermelon/151_100.jpg  ...  {'type': 1, 'size': None, 'indices': None, 'va...\n",
      "4    s3://sourceimage/Test/Raspberry/106_100.jpg  ...  {'type': 1, 'size': None, 'indices': None, 'va...\n",
      "\n",
      "[5 rows x 6 columns]"
     ]
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "zfyu7E2NnqwkwNaf2JqzsX",
     "report_properties": {
      "rowId": "bEr8dvfWQdvefvKxGqEXe3"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1280,)"
     ]
    }
   ],
   "source": [
    "df.loc[0,'features'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "LLevsTmEFCy74Pw3QdHjz9",
     "report_properties": {
      "rowId": "3warlwhj8CL8ZA8fhHm6lw"
     },
     "type": "CODE"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4685, 6)"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "85gKrKfXhWNttAxYVqsISe",
     "report_properties": {
      "rowId": "nmZQ5E9zxcOav2HNM3qQqt"
     },
     "type": "MD"
    }
   },
   "source": [
    "<u>On peut également constater la présence des fichiers <br />\n",
    "    au format \"**parquet**\" sur le **serveur S3**</u> :\n",
    "\n",
    "![Affichage des résultats sur S3](Deployez_un_model_dans_le_Cloud_AWS/img/S3_Results.png)\n",
    "\n",
    "## 4.11 Suivi de l'avancement des tâches avec le Serveur d'Historique Spark\n",
    "\n",
    "Il est possible de voir l'avancement des tâches en cours <br />\n",
    "avec le **serveur d'historique Spark**.\n",
    "\n",
    "![Accès au serveur d'historique spark](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_serveur_historique_spark_acces.png)\n",
    "\n",
    "**Il est également possible de revenir et d'étudier les tâches <br />\n",
    "qui ont été réalisé, afin de debugger, optimiser les futurs <br />\n",
    "tâches à réaliser.**\n",
    "\n",
    "<u>Lorsque la commande \"**features_df.write.mode(\"overwrite\").parquet(PATH_Result)**\" <br />\n",
    "était en cours, nous pouvions observer son état d'avancement</u> :\n",
    "\n",
    "![Progression execution script](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_jupyterhub_avancement.png)\n",
    "\n",
    "<u>Le **serveur d'historique Spark** nous permet une vision beaucoup plus précise <br />\n",
    "de l'exécution des différentes tâche sur les différentes machines du cluster</u> :\n",
    "\n",
    "![Suivi des tâches spark](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_SHSpark_01.png)\n",
    "\n",
    "On peut également constater que notre cluster de calcul a mis <br />\n",
    "un tout petit peu **moins de 8 minutes** pour traiter les **22 688 images**.\n",
    "\n",
    "![Temps de traitement](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_SHSpark_02.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "jce2XBNTaz2CPzahfxfLSc",
     "report_properties": {
      "rowId": "NINadV4pJwTJq8KFtEqSa5"
     },
     "type": "MD"
    }
   },
   "source": [
    "## 4.12 Résiliation de l'instance EMR\n",
    "\n",
    "Notre travail est maintenant terminé. <br />\n",
    "Le cluster de machines EMR est **facturé à la demande**, <br />\n",
    "et nous continuons d'être facturé même lorsque <br />\n",
    "les machines sont au repos.<br />\n",
    "Pour **optimiser la facturation**, il nous faut <br />\n",
    "maintenant **résilier le cluster**.\n",
    "\n",
    "<u>Je réalise cette commande depuis l'interface AWS</u> :\n",
    "\n",
    "1. Commencez par **désactiver le tunnel ssh dans FoxyProxy** pour éviter des problèmes de **timeout**.\n",
    "![Désactivation de FoxyProxy](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_foxyproxy_desactivation.png)\n",
    "2. Cliquez sur \"**Résilier**\"\n",
    "![Cliquez sur Résilier](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_resiliation_01.png)\n",
    "3. Confirmez la résiliation\n",
    "![Confirmez la résiliation](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_resiliation_02.png)\n",
    "4. La résiliation prend environ **1 minute**\n",
    "![Résiliation en cours](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_resiliation_03.png)\n",
    "5. La résiliation est effectuée\n",
    "![Résiliation terminée](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_resiliation_04.png)\n",
    "\n",
    "## 4.13 Cloner le serveur EMR (si besoin)\n",
    "\n",
    "Si nous devons de nouveau exécuter notre notebook dans les mêmes conditions, <br />\n",
    "il nous suffit de **cloner notre cluster** et ainsi en obtenir une copie fonctionnelle <br />\n",
    "sous 15/20 minutes, le temps de son instanciation.\n",
    "\n",
    "<u>Pour cela deux solutions</u> :\n",
    "1. <u>Depuis l'interface AWS</u> :\n",
    " 1. Cliquez sur \"**Cloner**\"\n",
    "   ![Cloner un cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_cloner_01.png)\n",
    " 2. Dans notre cas nous ne souhaitons pas inclure d'étapes\n",
    "   ![Ne pas inclure d'étapes](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_cloner_02.png)\n",
    " 3. La configuration du cluster est recréée à l’identique. <br />\n",
    "    On peut revenir sur les différentes étapes si on souhaite apporter des modifications<br />\n",
    "    Quand tout est prêt, cliquez sur \"**Créer un cluster**\"\n",
    "  ![Vérification/Modification/Créer un cluster](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_cloner_03.png)\n",
    "2. <u>En ligne de commande</u> (avec AWS CLI d'installé et de configuré et en s'assurant <br />\n",
    "   de s'attribuer les droits nécessaires sur le compte AMI utilisé)\n",
    " 1. Cliquez sur \"**Exporter AWS CLI**\"\n",
    " ![Exporter AWS CLI](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_cloner_cli_01.png)\n",
    " 2. Copier/Coller la commande **depuis un terminal**\n",
    " ![Copier Coller Commande](Deployez_un_model_dans_le_Cloud_AWS/img/EMR_cloner_cli_02.png)\n",
    "\n",
    "## 4.14 Arborescence du serveur S3 à la fin du projet\n",
    "\n",
    "<u>Pour information, voici **l'arborescence complète de mon bucket S3 p8-data** à la fin du projet</u> : <br />\n",
    "*Par soucis de lisibilité, je ne liste pas les 131 sous dossiers du répertoire \"Test\"*\n",
    "\n",
    "1. Results/_SUCCESS\n",
    "1. Results/part-00000-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00001-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00002-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00003-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00004-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00005-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00006-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00007-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00008-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00009-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00010-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00011-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00012-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00013-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00014-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00015-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00016-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00017-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00018-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00019-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00020-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00021-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00022-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Results/part-00023-2cc36f38-19ef-4d8a-a0d1-5ddb309b3894-c000.snappy.parquet\n",
    "1. Test/\n",
    "1. bootstrap-emr.sh\n",
    "1. jupyter-s3-conf.json\n",
    "1. jupyter/jovyan/.s3keep\n",
    "1. jupyter/jovyan/P8_01_Notebook.ipynb\n",
    "1. jupyter/jovyan/_metadata\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/file-perm.sqlite\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/html/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbconvert/templates/latex/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/nbsignatures.db\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.aws-editors-workspace-metadata/notebook_secret\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/Untitled1-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/.ipynb_checkpoints/test3-checkpoint.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/Untitled1.ipynb\n",
    "1. jupyter/jovyan/e-5OTY4VKPDT21945FF6DN15E35/test3.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "datalore": {
     "hide_input_from_viewers": false,
     "hide_output_from_viewers": false,
     "node_id": "PukHPePyqZsHNjffWXG2lW",
     "report_properties": {
      "rowId": "ArTjpkEkTiFdSrf3zeBDpF"
     },
     "type": "MD"
    }
   },
   "source": [
    "# 5. Conclusion\n",
    "\n",
    "Nous avons réalisé ce projet **en deux temps** en tenant <br />\n",
    "compte des contraintes qui nous ont été imposées.\n",
    "\n",
    "Nous avons **dans un premier temps développé notre solution en local** <br />\n",
    "sur une machine virtuelle dans un environnement Linux Ubuntu.\n",
    "\n",
    "La <u>première phase</u> a consisté à **installer l'environnement de travail Spark**. <br />\n",
    "**Spark** a un paramètre qui nous permet de travaillé en local et nous permet <br />\n",
    "ainsi de **simuler du calcul partagé** en considérant <br />\n",
    "**chaque cœur d'un processeur comme un worker indépendant**.<br />\n",
    "Nous avons travaillé sur un plus **petit jeu de donnée**, l'idée était <br />\n",
    "simplement de **valider le bon fonctionnement de la solution**.\n",
    "\n",
    "Nous avons fait le choix de réaliser du **transfert learning** <br />\n",
    "à partir du model **MobileNetV2**.<br />\n",
    "Ce modèle a été retenu pour sa **légèreté** et sa **rapidité d'exécution** <br />\n",
    "ainsi que pour la **faible dimension de son vecteur en sortie**.\n",
    "\n",
    "Les résultats ont été enregistrés sur disque en plusieurs <br />\n",
    "partitions au format \"**parquet**\".\n",
    "\n",
    "<u>**La solution a parfaitement fonctionné en mode local**</u>.\n",
    "\n",
    "La <u>deuxième phase</u> a consisté à créer un **réel cluster de calculs**. <br />\n",
    "L'objectif était de pouvoir **anticiper une future augmentation de la charge de travail**.\n",
    "\n",
    "Le meilleur choix retenu a été l'utilisation du prestataire de services **Amazon Web Services** <br />\n",
    "qui nous permet de **louer à la demande de la puissance de calculs**, <br />\n",
    "pour un **coût tout à fait acceptable**.<br />\n",
    "Ce service se nomme **EC2** et se classe parmi les offres **Infrastructure As A Service** (IAAS).\n",
    "\n",
    "Nous sommes allez plus loin en utilisant un service de plus <br />\n",
    "haut niveau (**Plateforme As A Service** PAAS)<br />\n",
    "en utilisant le service **EMR** qui nous permet d'un seul coup <br />\n",
    "d'**instancier plusieurs serveur (un cluster)** sur lesquels <br />\n",
    "nous avons pu demander l'installation et la configuration de plusieurs<br />\n",
    "programmes et librairies nécessaires à notre projet comme **Spark**, <br />\n",
    "**Hadoop**, **JupyterHub** ainsi que la librairie **TensorFlow**.\n",
    "\n",
    "En plus d'être plus **rapide et efficace à mettre en place**, nous avons <br />\n",
    "la **certitude du bon fonctionnement de la solution**, celle-ci ayant été <br />\n",
    "préalablement validé par les ingénieurs d'Amazon.\n",
    "\n",
    "Nous avons également pu installer, sans difficulté, **les packages <br />\n",
    "nécessaires sur l'ensembles des machines du cluster**.\n",
    "\n",
    "Enfin, avec très peu de modification, et plus simplement encore, <br />\n",
    "nous avons pu **exécuter notre notebook comme nous l'avions fait localement**.<br />\n",
    "Nous avons cette fois-ci exécuté le traitement sur **l'ensemble des images de notre dossier \"Test\"**.\n",
    "\n",
    "Nous avons opté pour le service **Amazon S3** pour **stocker les données de notre projet**. <br />\n",
    "S3 offre, pour un faible coût, toutes les conditions dont nous avons besoin pour stocker <br />\n",
    "et exploiter de manière efficace nos données.<br />\n",
    "L'espace alloué est potentiellement **illimité**, mais les coûts seront fonction de l'espace utilisé.\n",
    "\n",
    "Il nous sera **facile de faire face à une monté de la charge de travail** en **redimensionnant** <br />\n",
    "simplement notre cluster de machines (horizontalement et/ou verticalement au besoin), <br />\n",
    "les coûts augmenteront en conséquence mais resteront nettement inférieurs aux coûts engendrés <br />\n",
    "par l'achat de matériels ou par la location de serveurs dédiés."
   ]
  }
 ],
 "metadata": {
  "datalore": {
   "base_environment": "default",
   "computation_mode": "JUPYTER",
   "package_manager": "pip",
   "packages": [
    {
     "name": "sparkdl",
     "source": "PIP",
     "version": "0.2.2"
    },
    {
     "name": "tensorframes",
     "source": "PIP",
     "version": "0.2.9"
    },
    {
     "name": "findspark",
     "source": "PIP"
    },
    {
     "name": "spark",
     "source": "PIP"
    },
    {
     "name": "now",
     "source": "PIP",
     "version": "0.0.6"
    }
   ],
   "report_row_ids": [
    "NRE4THkPQvp5DIRltrNpV9",
    "ESjeOrKVMiDXPW51npSGNL",
    "b24rP3Ki3daHa8eUNL3zRI",
    "LEoj54KhvKnuCTswaL8eAU",
    "6LxNYUtbUBGnMbx33FPlq6",
    "iEIKIRMzrAJrZWUlgQ7EVp",
    "riqgh1CKWDXhV64jskDWSO",
    "li8QTVoU5WhpKYa3hUb2st",
    "o84yjCGwMnND50DCo0bfLy",
    "bNb0jl7O7gADqKxpwMjf0y",
    "PN4jmLjolbEzw06xql6QL4",
    "8RvaOppzFoRNK8WH05Ha4A",
    "8n6gxiawCrzz06idgofC50",
    "HVYmono8zYSlHEErQ7rUta",
    "R7QvJ6u5N2kKdx6UgFgGAe",
    "OoNTBltfCVtWqYFcM9n4Ux",
    "BqZ4kOvP90UUulI67ePBee",
    "sBnwZWYy9IaG1nkUv493YK",
    "cEbeJp2gTf920L6LJzZdLF",
    "P9az7rBvZ1Wbx8dusAGgkw",
    "KTExCjkL68rpINxVpoefwN",
    "5IDl1ej26nxt6rwrLAX38O",
    "inqw2oBQrWdpkBSTDiXcnZ",
    "MLNJuB5sIOMqsARn60hNZc",
    "vaXa65XFHi2pLSQQnfBkH2",
    "9xRg4EZqu8HK6r62mltiak",
    "NbhxnAPgrHPkkWxFvpMcAT",
    "JI30ppVzT4m1AQmYkmWf0d",
    "FrsvBjSWjyVWmNvCaFOCYE",
    "lqPhDUIrkTW3gD2gY6yHRq",
    "7Z4rJCFrbURpiff2PeZkRk",
    "tl2MphSQU31TY4KZAB2vgm",
    "2cqoVUI8NGXWBqWQB71Avu",
    "CA6v7VNVpBjCAunkJcN2hp",
    "df8K26Fb4AFQu977R8oJ5r",
    "Ti681pC8A0kDf8n45ROyyB",
    "w4GxYexKmQoAm2Mf2rx8zu",
    "3UuWIbJ7YAsYnahESI7jje",
    "VG7ei0eOlw5c2lube4SQzf",
    "cYNi5p3zHBiZuBo1E7MUOZ",
    "0XpyL8VBE0t5t9RbOx2l1t",
    "HIrYuHKyNkVyC0bRttZebP",
    "yj0PiVYrDLxpBbkzWnSphl",
    "mWjivhVKhHgKd5ZK3CAnWt",
    "HsXJNy67COMfybcJmqANGs",
    "b0zghCC4Pbfqwb2r3YQjF1",
    "B9MEriWMFBZRwwQtWv8EJE",
    "i4E1cr7pOM8HSSNIL4Ya39",
    "XIdOAnxk9aXRxsM4uEnIIR",
    "QYPrBpDvlf1xmq0DqaArM9",
    "6aR6SZqkA64cxqsK2gb9g8",
    "h4GU6Lckm52rAcmJf4GDPV",
    "rFqazkyGdybmJpKXOe3oRt",
    "ahyS4BipkPz8gvutSRrwCe",
    "oXyKqet752h3FTH3IJSpXb",
    "bSMuHHfW3q6GiRvo2PCfql",
    "d3PHlIehW9V1DjVKACqwpL",
    "EbJU2gdRYQ1TJbDCe1iR62",
    "wmHeRkXkeaNn2sNXO5M9tt",
    "SeecwiraXym9aa7KIc1s7U",
    "LYC50c5hhg9OtkEYwKk5Rh",
    "1nYLGXYbYYm2XnN4KYhAkO",
    "TtazPD6jovG3QYXbhExQNY",
    "lYNCrk1M5GsL8sgIOKP8uG",
    "0ZcLALXfofiV5eHslh0cTX",
    "Nvsc23j395Ue7WCHq1bmeh",
    "LMV8U4s9JvXYEhM2dSF3E7",
    "kfjRGehRqXwGaBzOIfBzJf",
    "wDuP4kuvtzt1vqUOPCbrDY",
    "mRy1dlsRmZXOOn8rcWkkes",
    "Ub3oMn1enIAAbUS1sVxFJa",
    "ktEHLkkUt5ZiKPLzGnjlZs",
    "m9AZ3KuaWchEH3SzdrukEF",
    "hQKazsF03wPZCI6WgORh96",
    "47wqTpXNARDyOnSuVkqYO4",
    "2ITpGIuI8GkLf8IeigxNCF",
    "nHtMWs5iHte7O9SEpM8YV1",
    "iNeQqfVtTz4UKSsLLjZVE6",
    "DvAM3qJnNlerX4U9A8FqWI",
    "JRf9dt5CGN1ptVcdJOXwDE",
    "3pcqirjM1PoVjFkwOfYB1m",
    "bEr8dvfWQdvefvKxGqEXe3",
    "3warlwhj8CL8ZA8fhHm6lw",
    "nmZQ5E9zxcOav2HNM3qQqt",
    "NINadV4pJwTJq8KFtEqSa5",
    "ArTjpkEkTiFdSrf3zeBDpF"
   ],
   "version": 3
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
